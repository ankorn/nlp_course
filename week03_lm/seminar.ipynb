{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "k1gpzj4guo8e1riwj3om1k"
   },
   "source": [
    "### N-gram language models or how to write scientific papers (4 pts)\n",
    "\n",
    "We shall train our language model on a corpora of [ArXiv](http://arxiv.org/) articles and see if we can generate a new one!\n",
    "\n",
    "![img](https://media.npr.org/assets/img/2013/12/10/istock-18586699-monkey-computer_brick-16e5064d3378a14e0e4c2da08857efe03c04695e-s800-c85.jpg)\n",
    "\n",
    "_data by neelshah18 from [here](https://www.kaggle.com/neelshah18/arxivdataset/)_\n",
    "\n",
    "_Disclaimer: this has nothing to do with actual science. But it's fun, so who cares?!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (2.4.1)\n",
      "Requirement already satisfied: wget in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (3.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: filelock in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib torch wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellId": "u8jdaiy68oib3jvr4k01"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellId": "0c76vnyl3zui9yhtkodgrlf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>day</th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>month</th>\n",
       "      <th>summary</th>\n",
       "      <th>tag</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38496</th>\n",
       "      <td>[{'name': 'Daniel Lemire'}]</td>\n",
       "      <td>24</td>\n",
       "      <td>cs/0605103v8</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>5</td>\n",
       "      <td>Time series are difficult to monitor, summariz...</td>\n",
       "      <td>[{'term': 'cs.DB', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>A Better Alternative to Piecewise Linear Time ...</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>[{'name': 'Subhadip Basu'}, {'name': 'Chitrita...</td>\n",
       "      <td>14</td>\n",
       "      <td>1202.3046v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>2</td>\n",
       "      <td>Character segmentation has long been one of th...</td>\n",
       "      <td>[{'term': 'cs.CV', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Segmentation of Offline Handwritten Bengali Sc...</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37016</th>\n",
       "      <td>[{'name': 'Bruno Woltzenlogel Paleo'}]</td>\n",
       "      <td>24</td>\n",
       "      <td>1603.07453v2</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>3</td>\n",
       "      <td>This paper argues that a combined treatment of...</td>\n",
       "      <td>[{'term': 'cs.LO', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>An Expressive Probabilistic Temporal Logic</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5471</th>\n",
       "      <td>[{'name': 'Kevin Swersky'}, {'name': 'Jasper S...</td>\n",
       "      <td>16</td>\n",
       "      <td>1406.3896v1</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>6</td>\n",
       "      <td>In this paper we develop a dynamic form of Bay...</td>\n",
       "      <td>[{'term': 'stat.ML', 'scheme': 'http://arxiv.o...</td>\n",
       "      <td>Freeze-Thaw Bayesian Optimization</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1624</th>\n",
       "      <td>[{'name': 'Chen Liang'}, {'name': 'Jonathan Be...</td>\n",
       "      <td>31</td>\n",
       "      <td>1611.00020v4</td>\n",
       "      <td>[{'rel': 'alternate', 'href': 'http://arxiv.or...</td>\n",
       "      <td>10</td>\n",
       "      <td>Harnessing the statistical power of neural net...</td>\n",
       "      <td>[{'term': 'cs.CL', 'scheme': 'http://arxiv.org...</td>\n",
       "      <td>Neural Symbolic Machines: Learning Semantic Pa...</td>\n",
       "      <td>2016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  author  day            id  \\\n",
       "38496                        [{'name': 'Daniel Lemire'}]   24  cs/0605103v8   \n",
       "4076   [{'name': 'Subhadip Basu'}, {'name': 'Chitrita...   14   1202.3046v1   \n",
       "37016             [{'name': 'Bruno Woltzenlogel Paleo'}]   24  1603.07453v2   \n",
       "5471   [{'name': 'Kevin Swersky'}, {'name': 'Jasper S...   16   1406.3896v1   \n",
       "1624   [{'name': 'Chen Liang'}, {'name': 'Jonathan Be...   31  1611.00020v4   \n",
       "\n",
       "                                                    link  month  \\\n",
       "38496  [{'rel': 'alternate', 'href': 'http://arxiv.or...      5   \n",
       "4076   [{'rel': 'alternate', 'href': 'http://arxiv.or...      2   \n",
       "37016  [{'rel': 'alternate', 'href': 'http://arxiv.or...      3   \n",
       "5471   [{'rel': 'alternate', 'href': 'http://arxiv.or...      6   \n",
       "1624   [{'rel': 'alternate', 'href': 'http://arxiv.or...     10   \n",
       "\n",
       "                                                 summary  \\\n",
       "38496  Time series are difficult to monitor, summariz...   \n",
       "4076   Character segmentation has long been one of th...   \n",
       "37016  This paper argues that a combined treatment of...   \n",
       "5471   In this paper we develop a dynamic form of Bay...   \n",
       "1624   Harnessing the statistical power of neural net...   \n",
       "\n",
       "                                                     tag  \\\n",
       "38496  [{'term': 'cs.DB', 'scheme': 'http://arxiv.org...   \n",
       "4076   [{'term': 'cs.CV', 'scheme': 'http://arxiv.org...   \n",
       "37016  [{'term': 'cs.LO', 'scheme': 'http://arxiv.org...   \n",
       "5471   [{'term': 'stat.ML', 'scheme': 'http://arxiv.o...   \n",
       "1624   [{'term': 'cs.CL', 'scheme': 'http://arxiv.org...   \n",
       "\n",
       "                                                   title  year  \n",
       "38496  A Better Alternative to Piecewise Linear Time ...  2006  \n",
       "4076   Segmentation of Offline Handwritten Bengali Sc...  2012  \n",
       "37016         An Expressive Probabilistic Temporal Logic  2016  \n",
       "5471                   Freeze-Thaw Bayesian Optimization  2014  \n",
       "1624   Neural Symbolic Machines: Learning Semantic Pa...  2016  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Alternative manual download link: https://yadi.sk/d/_nGyU2IajjR9-w\n",
    "# !wget \"https://www.dropbox.com/s/99az9n1b57qkd9j/arxivData.json.tar.gz?dl=1\" -O arxivData.json.tar.gz\n",
    "# !tar -xvzf arxivData.json.tar.gz\n",
    "data = pd.read_json(\"./arxivData.json\")\n",
    "data.sample(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellId": "lbyqb5rx7j8jpo591r06ak"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Differential Contrastive Divergence ; This paper has been retracted.',\n",
       " 'What Does Artificial Life Tell Us About Death? ; Short philosophical essay',\n",
       " 'P=NP ; We claim to resolve the P=?NP problem via a formal argument for P=NP.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assemble lines: concatenate title and description\n",
    "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
    "\n",
    "sorted(lines, key=len)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "7u97m5s8ekl5zd5a43a1yc"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "You know the dril. The data is messy. Go clean the data. Use WordPunctTokenizer or something.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellId": "u8rvfk719iek97t3rarwr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.WordPunctTokenizer()\n",
    "\n",
    "lines = [' '.join(tokenizer.tokenize(line)).lower() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellId": "w88nddpp2k8edoeyyyjh0l"
   },
   "outputs": [],
   "source": [
    "assert sorted(lines, key=len)[0] == \\\n",
    "    'differential contrastive divergence ; this paper has been retracted .'\n",
    "assert sorted(lines, key=len)[2] == \\\n",
    "    'p = np ; we claim to resolve the p =? np problem via a formal argument for p = np .'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "qb6h3hxmr095egzv8rlzul"
   },
   "source": [
    "### N-Gram Language Model (1point)\n",
    "\n",
    "A language model is a probabilistic model that estimates text probability: the joint probability of all tokens $w_t$ in text $X$: $P(X) = P(w_1, \\dots, w_T)$.\n",
    "\n",
    "It can do so by following the chain rule:\n",
    "$$P(w_1, \\dots, w_T) = P(w_1)P(w_2 \\mid w_1)\\dots P(w_T \\mid w_1, \\dots, w_{T-1}).$$ \n",
    "\n",
    "The problem with such approach is that the final term $P(w_T \\mid w_1, \\dots, w_{T-1})$ depends on $n-1$ previous words. This probability is impractical to estimate for long texts, e.g. $T = 1000$.\n",
    "\n",
    "One popular approximation is to assume that next word only depends on a finite amount of previous words:\n",
    "\n",
    "$$P(w_t \\mid w_1, \\dots, w_{t - 1}) = P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1})$$\n",
    "\n",
    "Such model is called __n-gram language model__ where n is a parameter. For example, in 3-gram language model, each word only depends on 2 previous words. \n",
    "\n",
    "$$\n",
    "    P(w_1, \\dots, w_n) = \\prod_t P(w_t \\mid w_{t - n + 1}, \\dots, w_{t - 1}).\n",
    "$$\n",
    "\n",
    "You can also sometimes see such approximation under the name of _n-th order markov assumption_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "u68wydbiioqlp5gl96mhd"
   },
   "source": [
    "The first stage to building such a model is counting all word occurences given N-1 previous words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellId": "og84gjipnumsakhiiu9ap"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# special tokens: \n",
    "# - `UNK` represents absent tokens, \n",
    "# - `EOS` is a special token after the end of sequence\n",
    "\n",
    "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
    "\n",
    "def count_ngrams(lines: list[str], n: int):\n",
    "    \"\"\"\n",
    "    Count how many times each word occured after (n - 1) previous words\n",
    "    :param lines: an iterable of strings with space-separated tokens\n",
    "    :returns: a dictionary { tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}\n",
    "\n",
    "    When building counts, please consider the following two edge cases:\n",
    "    - if prefix is shorter than (n - 1) tokens, it should be padded with UNK. For n=3,\n",
    "      empty prefix: \"\" -> (UNK, UNK)\n",
    "      short prefix: \"the\" -> (UNK, the)\n",
    "      long prefix: \"the new approach\" -> (new, approach)\n",
    "    - you should add a special token, EOS, at the end of each sequence\n",
    "      \"... with deep neural networks .\" -> (..., with, deep, neural, networks, ., EOS)\n",
    "      count the probability of this token just like all others.\n",
    "    \"\"\"\n",
    "    counts = defaultdict(Counter)\n",
    "    # counts[(word1, word2)][word3] = how many times word3 occured after (word1, word2)\n",
    "    \n",
    "    for line in lines:\n",
    "      line_list = line.split(' ')\n",
    "      line_list.append(EOS)\n",
    "      \n",
    "      for i, token in enumerate(line_list):\n",
    "        prefix = line_list[:i]\n",
    "        prefix = prefix[max(0, len(prefix) - n + 1):]\n",
    "        prefix = [UNK] * (n - 1 - len(prefix)) + prefix\n",
    "        prefix = tuple(prefix)\n",
    "          \n",
    "        token_count = 1 if prefix not in counts else counts[prefix][token] + 1\n",
    "          \n",
    "        counts[prefix][token] = token_count\n",
    "          \n",
    "        \n",
    "    return counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellId": "xyf2he6lak9mmqarl3nck"
   },
   "outputs": [],
   "source": [
    "# let's test it\n",
    "dummy_lines = sorted(lines, key=len)[:100]\n",
    "dummy_counts = count_ngrams(dummy_lines, n=3)\n",
    "assert set(map(len, dummy_counts.keys())) == {2}, \"please only count {n-1}-grams\"\n",
    "assert len(dummy_counts[('_UNK_', '_UNK_')]) == 78\n",
    "assert dummy_counts['_UNK_', 'a']['note'] == 3\n",
    "assert dummy_counts['p', '=']['np'] == 2\n",
    "assert dummy_counts['author', '.']['_EOS_'] == 1\n",
    "single_counts = count_ngrams(dummy_lines, n=1)\n",
    "assert single_counts[()][EOS] == len(dummy_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "4j620npeqvj0k8ak8xqx8xk"
   },
   "source": [
    "Once we can count N-grams, we can build a probabilistic language model.\n",
    "The simplest way to compute probabilities is in proporiton to counts:\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) \\over \\sum_{\\hat w} Count(prefix, \\hat w) } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellId": "c7cm76wmzlaa12bctznzei"
   },
   "outputs": [],
   "source": [
    "class NGramLanguageModel:    \n",
    "    def __init__(self, lines, n):\n",
    "        \"\"\" \n",
    "        Train a simple count-based language model: \n",
    "        compute probabilities P(w_t | prefix) given ngram counts\n",
    "        \n",
    "        :param n: computes probability of next token given (n - 1) previous words\n",
    "        :param lines: an iterable of strings with space-separated tokens\n",
    "        \"\"\"\n",
    "        assert n >= 1\n",
    "        self.n = n\n",
    "    \n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        \n",
    "        # compute token proabilities given counts\n",
    "        self.probs = defaultdict(Counter)\n",
    "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
    "        \n",
    "        # populate self.probs with actual probabilities\n",
    "        for prefix_key in counts:\n",
    "            sum = 0\n",
    "            for token in counts[prefix_key]:\n",
    "                sum += counts[prefix_key][token]\n",
    "            \n",
    "            for token in counts[prefix_key]:\n",
    "                self.probs[prefix_key][token] = counts[prefix_key][token] / sum\n",
    "        \n",
    "            \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :returns: a dictionary {token : it's probability} for all tokens with positive probabilities\n",
    "        \"\"\"\n",
    "        prefix = prefix.split()\n",
    "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
    "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
    "        return self.probs[tuple(prefix)]\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        \"\"\"\n",
    "        :param prefix: string with space-separated prefix tokens\n",
    "        :param next_token: the next token to predict probability for\n",
    "        :returns: P(next_token|prefix) a single number, 0 <= P <= 1\n",
    "        \"\"\"\n",
    "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "0ftnn4nmuzrup6c0vvhb8q"
   },
   "source": [
    "Let's test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellId": "a7zajcnvhqupvcrmacvkur"
   },
   "outputs": [],
   "source": [
    "dummy_lm = NGramLanguageModel(dummy_lines, n=3)\n",
    "\n",
    "p_initial = dummy_lm.get_possible_next_tokens('') # '' -> ['_UNK_', '_UNK_']\n",
    "assert np.allclose(p_initial['learning'], 0.02)\n",
    "assert np.allclose(p_initial['a'], 0.13)\n",
    "assert np.allclose(p_initial.get('meow', 0), 0)\n",
    "assert np.allclose(sum(p_initial.values()), 1)\n",
    "\n",
    "p_a = dummy_lm.get_possible_next_tokens('a') # '' -> ['_UNK_', 'a']\n",
    "assert np.allclose(p_a['machine'], 0.15384615)\n",
    "assert np.allclose(p_a['note'], 0.23076923)\n",
    "assert np.allclose(p_a.get('the', 0), 0)\n",
    "assert np.allclose(sum(p_a.values()), 1)\n",
    "\n",
    "assert np.allclose(dummy_lm.get_possible_next_tokens('a note')['on'], 1)\n",
    "assert dummy_lm.get_possible_next_tokens('a machine') == \\\n",
    "    dummy_lm.get_possible_next_tokens(\"there have always been ghosts in a machine\"), \\\n",
    "    \"your 3-gram model should only depend on 2 previous words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oh8r9a41kuk4r51wra9"
   },
   "source": [
    "Now that you've got a working n-gram language model, let's see what sequences it can generate. But first, let's train it on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cellId": "f17xoejjppmooo2nopw4xo"
   },
   "outputs": [],
   "source": [
    "lm = NGramLanguageModel(lines, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2kd9glwnkr470qc4bt7f1e"
   },
   "source": [
    "The process of generating sequences is... well, it's sequential. You maintain a list of tokens and iteratively add next token by sampling with probabilities.\n",
    "\n",
    "$ X = [] $\n",
    "\n",
    "__forever:__\n",
    "* $w_{next} \\sim P(w_{next} | X)$\n",
    "* $X = concat(X, w_{next})$\n",
    "\n",
    "\n",
    "Instead of sampling with probabilities, one can also try always taking most likely token, sampling among top-K most likely tokens or sampling with temperature. In the latter case (temperature), one samples from\n",
    "\n",
    "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
    "\n",
    "Where $\\tau > 0$ is model temperature. If $\\tau << 1$, more likely tokens will be sampled with even higher probability while less likely tokens will vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellId": "sgbatlm9vzb4z889fho7"
   },
   "outputs": [],
   "source": [
    "def get_next_token(lm: NGramLanguageModel, prefix, temperature=1.0):\n",
    "    \"\"\"\n",
    "    return next token after prefix;\n",
    "    :param temperature: samples proportionally to lm probabilities ^ (1 / temperature)\n",
    "        if temperature == 0, always takes most likely token. Break ties arbitrarily.\n",
    "    \"\"\"\n",
    "    probs = lm.get_possible_next_tokens(prefix)\n",
    "    \n",
    "    if temperature == 0.0:\n",
    "        token, prob = sorted(probs.most_common(), key=lambda s: s[1], reverse=True)[0]\n",
    "        return token\n",
    "    \n",
    "    tokens, probs = zip(*probs.items())\n",
    "    \n",
    "    probs = [(prob ** (1 / temperature)) for prob in probs]\n",
    "    \n",
    "    probs_sum = sum(probs)\n",
    "    \n",
    "    probs = [prob / probs_sum for prob in probs]\n",
    "        \n",
    "    return np.random.choice(tokens, p=probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellId": "98l40131wjtd5xbdm5b2nr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks nice!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
    "assert 250 < test_freqs['not'] < 450\n",
    "assert 8500 < test_freqs['been'] < 9500\n",
    "assert 1 < test_freqs['lately'] < 200\n",
    "\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
    "assert 1500 < test_freqs['learning'] < 3000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
    "assert 8000 < test_freqs['learning'] < 9000\n",
    "test_freqs = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
    "assert test_freqs['learning'] == 10000\n",
    "\n",
    "print(\"Looks nice!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ux4n8iq523n4s3ftrelhxj"
   },
   "source": [
    "Let's have fun with this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cellId": "1nnnycga61rijt6nd8zai"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artificial neural networks perform well on the object in just a quarter } of w from the univariate optimization objective for selecting k gaussian features from a single - agent scenarios , one of the art prediction of consumer debt has risen from the easy training examples . such evaluation is performed by artificial neural networks and demystifying the intrinsic dimensionality . to do approximate nearest neighbor search ; the massive amount of data , the top layers of four qos parameters . an area under the framework of approximate policy iteration : ( 1 ) this constraint can be approximated\n"
     ]
    }
   ],
   "source": [
    "prefix = 'artificial' # <- your ideas :)\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cellId": "pxyjsv3b7r8thdfxlgitl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the gap between the proposal of a word - level features . we propose a novel approach to automatic evaluation metrics . _EOS_\n"
     ]
    }
   ],
   "source": [
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "2n90bscmzfko0qnctp7ysc"
   },
   "source": [
    "__More in the homework:__ nucleus sampling, top-k sampling, beam search(not for the faint of heart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3gdmey7g8at5n5c5x4gayh"
   },
   "source": [
    "### Evaluating language models: perplexity (1point)\n",
    "\n",
    "Perplexity is a measure of how well your model approximates the true probability distribution behind the data. __Smaller perplexity = better model__.\n",
    "\n",
    "To compute perplexity on one sentence, use:\n",
    "$$\n",
    "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
    "$$\n",
    "\n",
    "\n",
    "On the corpora level, perplexity is a product of probabilities of all tokens in all sentences to the power of $1/N$, where $N$ is __total length (in tokens) of all sentences__ in corpora.\n",
    "\n",
    "This number can quickly get too small for float32/float64 precision, so we recommend you to first compute log-perplexity (from log-probabilities) and then take the exponent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "cellId": "5hp010xyzzb4vqewo1bhny"
   },
   "outputs": [],
   "source": [
    "def perplexity(lm: NGramLanguageModel, lines: list[str], min_logprob=np.log(10 ** -50.)):\n",
    "    \"\"\"\n",
    "    :param lines: a list of strings with space-separated tokens\n",
    "    :param min_logprob: if log(P(w | ...)) is smaller than min_logprop, set it equal to min_logrob\n",
    "    :returns: corpora-level perplexity - a single scalar number from the formula above\n",
    "    \n",
    "    Note: do not forget to compute P(w_first | empty) and P(eos | full_sequence)\n",
    "    \n",
    "    PLEASE USE lm.get_next_token_prob and NOT lm.get_possible_next_tokens\n",
    "    \"\"\"\n",
    "    lines = [line + ' ' + EOS for line in lines]\n",
    "    \n",
    "    total_len = sum([len(line.split(' ')) for line in lines])\n",
    "    \n",
    "    log_probs_sums = 0\n",
    "    for sentence in lines:\n",
    "        sentence_list = sentence.split(' ')\n",
    "        for i, token in enumerate(sentence_list):\n",
    "            prefix = ' '.join(sentence_list[:i])\n",
    "            \n",
    "            p = lm.get_next_token_prob(prefix, token)\n",
    "            \n",
    "            if p == 0:\n",
    "                p = 10 ** -50.\n",
    "            \n",
    "            log_p = np.log(p)\n",
    "            if log_p < min_logprob:\n",
    "                log_p = min_logprob\n",
    "                \n",
    "            log_probs_sums += log_p\n",
    "            \n",
    "    total_log_probs_sums = np.sum(log_probs_sums)\n",
    "    \n",
    "    return np.exp((-(1 / total_len) * total_log_probs_sums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellId": "8b689bobhkey04x7pabupj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexities: ppx1=318.213 ppx3=1.520 ppx10=1.184\n"
     ]
    }
   ],
   "source": [
    "lm1 = NGramLanguageModel(dummy_lines, n=1)\n",
    "lm3 = NGramLanguageModel(dummy_lines, n=3)\n",
    "lm10 = NGramLanguageModel(dummy_lines, n=10)\n",
    "\n",
    "ppx1 = perplexity(lm1, dummy_lines)\n",
    "ppx3 = perplexity(lm3, dummy_lines)\n",
    "ppx10 = perplexity(lm10, dummy_lines)\n",
    "ppx_missing = perplexity(lm3, ['the jabberwock , with eyes of flame , '])  # thanks, L. Carrol\n",
    "\n",
    "print(\"Perplexities: ppx1=%.3f ppx3=%.3f ppx10=%.3f\" % (ppx1, ppx3, ppx10))\n",
    "\n",
    "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10)), \"perplexity should be non-negative and reasonably small\"\n",
    "assert ppx1 > ppx3 > ppx10, \"higher N models should overfit and \"\n",
    "assert np.isfinite(ppx_missing) and ppx_missing > 10 ** 6, \"missing words should have large but finite perplexity. \" \\\n",
    "    \" Make sure you use min_logprob right\"\n",
    "assert np.allclose([ppx1, ppx3, ppx10], (318.2132342216302, 1.5199996213739575, 1.1838145037901249))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "ypc4lks4vs1li908fqi8"
   },
   "source": [
    "Now let's measure the actual perplexity: we'll split the data into train and test and score model on test data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellId": "tjnehsem2lmijkg2lto4w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 1832.23136\n",
      "N = 2, Perplexity = 85653987.28774\n",
      "N = 3, Perplexity = 61999196259042902147072.00000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
    "\n",
    "for n in (1, 2, 3):\n",
    "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cellId": "38nfbfkpzgfxik8kccyt1l"
   },
   "outputs": [],
   "source": [
    "# whoops, it just blew up :)\n",
    "# no it didn't!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "oopn2o57wxm9vbxzycytce"
   },
   "source": [
    "### LM Smoothing\n",
    "\n",
    "The problem with our simple language model is that whenever it encounters an n-gram it has never seen before, it assigns it with the probabilitiy of 0. Every time this happens, perplexity explodes.\n",
    "\n",
    "To battle this issue, there's a technique called __smoothing__. The core idea is to modify counts in a way that prevents probabilities from getting too low. The simplest algorithm here is Additive smoothing (aka [Lapace smoothing](https://en.wikipedia.org/wiki/Additive_smoothing)):\n",
    "\n",
    "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$\n",
    "\n",
    "If counts for a given prefix are low, additive smoothing will adjust probabilities to a more uniform distribution. Not that the summation in the denominator goes over _all words in the vocabulary_.\n",
    "\n",
    "Here's an example code we've implemented for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cellId": "ioh26rlov6g8l2ssj1c8pm"
   },
   "outputs": [],
   "source": [
    "class LaplaceLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" this code is an example, no need to change anything \"\"\"\n",
    "    def __init__(self, lines, n, delta=1.0):\n",
    "        self.n = n\n",
    "        counts = count_ngrams(lines, self.n)\n",
    "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
    "        self.probs = defaultdict(Counter)\n",
    "\n",
    "        for prefix in counts:\n",
    "            token_counts = counts[prefix]\n",
    "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
    "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
    "                                          for token in token_counts}\n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
    "    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "            missing_prob_total = max(0, missing_prob_total) # prevent rounding errors\n",
    "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "90vsann3920ie05r2blbmi",
    "execution_id": "3868303d-0bb9-42c6-a9a8-dcf485c8220c"
   },
   "source": [
    "**Disclaimer**: the implementation above assumes all words unknown within a given context to be equally likely, *as well as the words outside of vocabulary*. Therefore, its' perplexity will be lower than it should when encountering such words. Therefore, comparing it with a model with fewer unknown words will not be fair. When implementing your own smoothing, you may handle this by adding a virtual `UNK` token of non-zero probability. Technically, this will result in a model where probabilities do not add up to $1$, but it is close enough for a practice excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cellId": "3xvxkdxcmfqucruyt66mdc"
   },
   "outputs": [],
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1, 2, 3):\n",
    "    dummy_lm = LaplaceLanguageModel(dummy_lines, n=n)\n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellId": "j6zqa50koitjjri9ipd8ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 1, Perplexity = 977.67559\n",
      "N = 2, Perplexity = 470.48021\n",
      "N = 3, Perplexity = 3679.44765\n"
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cellId": "pjuqt30jcerwbz1ym9zv1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bridging the lapses teuscher presupposing gr clickture deconstruct hvs linearization ancestor sediments mores nociceptive affective ostensibly oxford conciseness inspection prices perdurantism gfan reconsider 3750 konieczny hpid prunes intégration carlo latin opted wsvt mentation whisk ascribed freed plumb might hirst privatizing workshop handoverface _db1 secoc experiments deepinsight mtlr wav2letter discs regents canteens friendliness dbdesigner jumping expenses need lepidoptera dsclrcn zslices y_ robddc |^ bsd400 procuring donsvic influencer qnnae tenders scop lim_ conceptuelle guessed riley inflate voluntary minutia odpd packet frontline jihadist house ^*$) inflammation spels melt diversely eiphstein rrw profanity gflops nferm lam dibco undertaking piecwise pml feature malagasy breakhis errorneous unitized hydrocyclone\n"
     ]
    }
   ],
   "source": [
    "# optional: try to sample tokens from such a model\n",
    "lm = LaplaceLanguageModel(lines, n=3, delta=0.1)\n",
    "\n",
    "prefix = 'bridging the' # <- more of your ideas\n",
    "\n",
    "for i in range(100):\n",
    "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
    "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
    "        break\n",
    "        \n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cellId": "3b8s1y9uls4fosu3yp28gg"
   },
   "source": [
    "### Kneser-Ney smoothing (2 points)\n",
    "\n",
    "Additive smoothing is simple, reasonably good but definitely not a State of The Art algorithm.\n",
    "\n",
    "\n",
    "Your final task in this notebook is to implement [Kneser-Ney](https://en.wikipedia.org/wiki/Kneser%E2%80%93Ney_smoothing) smoothing.\n",
    "\n",
    "It can be computed recurrently, for n>1:\n",
    "\n",
    "$$P_{kn}(w_t | prefix_{n-1}) = { \\max(0, Count(prefix_{n-1}, w_t) - \\delta) \\over \\sum_{\\hat w} Count(prefix_{n-1}, \\hat w)} + \\lambda_{prefix_{n-1}} \\cdot P_{kn}(w_t | prefix_{n-2})$$\n",
    "\n",
    "where\n",
    "- $prefix_{n-1}$ is a tuple of {n-1} previous tokens\n",
    "- $lambda_{prefix_{n-1}}$ is a normalization constant chosen so that probabilities add up to 1\n",
    "- Unigram $P_{kn}(w_t | prefix_{n-2})$ corresponds to Kneser Ney smoothing for {N-1}-gram language model.\n",
    "- Unigram $P_{kn}(w_t)$ is a special case: how likely it is to see x_t in an unfamiliar context\n",
    "\n",
    "See lecture slides or wiki for more detailed formulae.\n",
    "\n",
    "__Your task__ is to\n",
    "- implement `KneserNeyLanguageModel` class,\n",
    "- test it on 1-3 gram language models\n",
    "- find optimal (within reason) smoothing delta for 3-gram language model with Kneser-Ney smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "cellId": "2ix7kzw02v30oye55322all"
   },
   "outputs": [],
   "source": [
    "class KneserNeyLanguageModel(NGramLanguageModel): \n",
    "    \"\"\" A template for Kneser-Ney language model. Default delta may be suboptimal. \"\"\"\n",
    "    def __init__(self, lines: list[str], n, delta=1.0):\n",
    "        self.n = n\n",
    "        self.delta = delta\n",
    "        self.corpora_list = ' '.join([line + ' ' + EOS for line in lines]).split(' ')\n",
    "        self.corpora_count = Counter(self.corpora_list)\n",
    "        \n",
    "        self.n_grams_counts = {}\n",
    "        for current_n in range(1, 4):\n",
    "            self.n_grams_counts[current_n] = count_ngrams(lines, current_n)\n",
    "            \n",
    "        self.distinct_pairs_count = 0\n",
    "        for prefix in self.n_grams_counts[2]:\n",
    "            for token in self.n_grams_counts[2][prefix]:\n",
    "                if prefix != (UNK,):\n",
    "                    self.distinct_pairs_count += 1\n",
    "                    \n",
    "        self.vocab = set(token for token_counts in self.n_grams_counts[n].values() for token in token_counts)\n",
    "            \n",
    "        self.probs = defaultdict(Counter)\n",
    "        for prefix in self.n_grams_counts[n]:\n",
    "            for token in self.n_grams_counts[n][prefix]:\n",
    "                self.probs[prefix][token] = self.get_prob(prefix, token)\n",
    "                \n",
    "    def get_prob(self, prefix: tuple, token: str):\n",
    "        prefix_len = len(prefix)\n",
    "        \n",
    "        if prefix_len == 0:\n",
    "            amount_of_unique_words_the_unigram_follows = self.n_grams_counts[1][prefix][token] - self.n_grams_counts[2][(UNK,)][token]\n",
    "\n",
    "            print(token, amount_of_unique_words_the_unigram_follows)\n",
    "            \n",
    "            return amount_of_unique_words_the_unigram_follows / self.distinct_pairs_count\n",
    "        \n",
    "        counts = self.n_grams_counts[prefix_len + 1]\n",
    "        \n",
    "        token_prefix_counts_sum = 0\n",
    "        token_prefix_amount = 0\n",
    "        for _token in counts[prefix]:\n",
    "            token_prefix_counts_sum += counts[prefix][_token]\n",
    "            token_prefix_amount += 1\n",
    "        \n",
    "        operand_1 = max(counts[prefix][token] - self.delta, 0) / token_prefix_counts_sum\n",
    "        operand_2 = self.delta * (token_prefix_amount / token_prefix_counts_sum)\n",
    "        decrement_n_prob = self.get_prob(prefix[1:], token)\n",
    "        return operand_1 + (operand_2 * decrement_n_prob)\n",
    "    \n",
    "    def get_possible_next_tokens(self, prefix):\n",
    "        token_probs = super().get_possible_next_tokens(prefix)\n",
    "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
    "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
    "        \n",
    "        return { token: token_probs.get(token, missing_prob) for token in self.vocab }\n",
    "                    \n",
    "    def get_next_token_prob(self, prefix, next_token):\n",
    "        token_probs = self.get_possible_next_tokens(prefix)\n",
    "        # print(token_probs)\n",
    "        \n",
    "        if next_token in token_probs:\n",
    "            return token_probs[next_token]\n",
    "        else:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what 0\n",
      "does 1\n",
      "artificial 1\n",
      "life 1\n",
      "tell 1\n",
      "us 1\n",
      "about 1\n",
      "death 1\n",
      "? 1\n",
      "; 1\n",
      "short 1\n",
      "philosophical 1\n",
      "essay 1\n",
      "_EOS_ 1\n",
      "0.9999999999999998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line_1 = 'what does artificial life tell us about death ? ; short philosophical essay' # ok\n",
    "line_2 = 'p = np qwe ; we claim to resolve the p =? np problem via a formal argument for p = np .' # not ok!\n",
    "model = KneserNeyLanguageModel([line_1], n=1)\n",
    "\n",
    "x = sum([model.get_next_token_prob('prefix is ignored for n=1', token) for token in model.vocab])\n",
    "print(x)\n",
    "np.allclose(x, 1)\n",
    "\n",
    "# TODO: отладить вероятности юниграмм для простейшей строки\n",
    "\n",
    "# TODO: это ошибка накопления неточности при делении?\n",
    "# возможно, пример сглаживания лапласа был дан, чтобы показать, как с этим справляться\n",
    "\n",
    "# TODO: разобраться всё-таки в P_continuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram: 1.3791946308724925\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "I told you not to break anything! :)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m dummy_lm \u001b[38;5;241m=\u001b[39m KneserNeyLanguageModel(dummy_lines, n\u001b[38;5;241m=\u001b[39mn)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-gram:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28msum\u001b[39m([dummy_lm\u001b[38;5;241m.\u001b[39mget_next_token_prob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, w_i) \u001b[38;5;28;01mfor\u001b[39;00m w_i \u001b[38;5;129;01min\u001b[39;00m dummy_lm\u001b[38;5;241m.\u001b[39mvocab]))\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(\u001b[38;5;28msum\u001b[39m([dummy_lm\u001b[38;5;241m.\u001b[39mget_next_token_prob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, w_i) \u001b[38;5;28;01mfor\u001b[39;00m w_i \u001b[38;5;129;01min\u001b[39;00m dummy_lm\u001b[38;5;241m.\u001b[39mvocab]), \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI told you not to break anything! :)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: I told you not to break anything! :)"
     ]
    }
   ],
   "source": [
    "#test that it's a valid probability model\n",
    "for n in (1,): # 2, 3):\n",
    "    dummy_lm = KneserNeyLanguageModel(dummy_lines, n=n)\n",
    "   \n",
    "    print(f'{n}-gram:', sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]))\n",
    "        \n",
    "    assert np.allclose(sum([dummy_lm.get_next_token_prob('a', w_i) for w_i in dummy_lm.vocab]), 1), \"I told you not to break anything! :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "cellId": "pp3jtkk9annp1qkou58x1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faster\n",
      "fuzzing\n",
      ":\n",
      "reinitialization\n",
      "with\n",
      "deep\n",
      "neural\n",
      "models\n",
      ";\n",
      "we\n",
      "improve\n",
      "the\n",
      "276197\n",
      "performance\n",
      "of\n",
      "american\n",
      "fuzzy\n",
      "lop\n",
      "(\n",
      "afl\n",
      ")\n",
      "fuzz\n",
      "testing\n",
      "framework\n",
      "by\n",
      "using\n",
      "generative\n",
      "adversarial\n",
      "network\n",
      "gan\n",
      "to\n",
      "reinitialize\n",
      "system\n",
      "novel\n",
      "seed\n",
      "files\n",
      ".\n",
      "assess\n",
      "based\n",
      "on\n",
      "temporal\n",
      "rate\n",
      "at\n",
      "which\n",
      "produce\n",
      "and\n",
      "unseen\n",
      "code\n",
      "paths\n",
      "compare\n",
      "this\n",
      "approach\n",
      "file\n",
      "generation\n",
      "from\n",
      "a\n",
      "random\n",
      "draw\n",
      "bytes\n",
      "observed\n",
      "in\n",
      "training\n",
      "path\n",
      "lengths\n",
      "variations\n",
      "were\n",
      "not\n",
      "sufficiently\n",
      "diverse\n",
      "fully\n",
      "replace\n",
      "input\n",
      "however\n",
      ",\n",
      "augmenting\n",
      "native\n",
      "these\n",
      "additional\n",
      "demonstrated\n",
      "improvements\n",
      "over\n",
      "alone\n",
      "specifically\n",
      "experiments\n",
      "showed\n",
      "was\n",
      "more\n",
      "effective\n",
      "than\n",
      "lstm\n",
      "out\n",
      "-\n",
      "performed\n",
      "augmentation\n",
      "strategy\n",
      "as\n",
      "measured\n",
      "number\n",
      "unique\n",
      "discovered\n",
      "helps\n",
      "discover\n",
      "14\n",
      "23\n",
      "%\n",
      "same\n",
      "amount\n",
      "cpu\n",
      "time\n",
      "finds\n",
      "6\n",
      "16\n",
      "that\n",
      "are\n",
      "average\n",
      "13\n",
      "84\n",
      "longer\n",
      "shows\n",
      "promise\n",
      "for\n",
      "help\n",
      "fuzzer\n",
      "exercise\n",
      "software\n",
      "_EOS_\n",
      "optimization\n",
      "quantum\n",
      "reinforcement\n",
      "algorithm\n",
      "solves\n",
      "classical\n",
      "problem\n",
      "introducing\n",
      "feedback\n",
      "slowly\n",
      "changes\n",
      "energy\n",
      "landscape\n",
      "converges\n",
      "an\n",
      "optimal\n",
      "solution\n",
      "configuration\n",
      "space\n",
      "here\n",
      "use\n",
      "concentrate\n",
      "localize\n",
      "preferentially\n",
      "wave\n",
      "function\n",
      "particle\n",
      "explores\n",
      "examine\n",
      "method\n",
      "solving\n",
      "numerically\n",
      "equations\n",
      "governing\n",
      "evolution\n",
      "similar\n",
      "nonlinear\n",
      "schr\n",
      "\\\"\n",
      "odinger\n",
      "small\n",
      "sizes\n",
      "particular\n",
      "observe\n",
      "increases\n",
      "minimal\n",
      "gap\n",
      "annealing\n",
      "our\n",
      "numerical\n",
      "simulations\n",
      "latter\n",
      "observation\n",
      "show\n",
      "such\n",
      "kind\n",
      "feedbacks\n",
      "might\n",
      "be\n",
      "helpful\n",
      "computationally\n",
      "hard\n",
      "nature\n",
      "inspired\n",
      "artificial\n",
      "intelligence\n",
      "adaptive\n",
      "traffic\n",
      "flow\n",
      "distribution\n",
      "computer\n",
      "because\n",
      "stochastic\n",
      "requirement\n",
      "matrix\n",
      "it\n",
      "is\n",
      "very\n",
      "difficult\n",
      "get\n",
      "minimize\n",
      "delay\n",
      "even\n",
      "routing\n",
      "protocol\n",
      "fixed\n",
      "connection\n",
      "where\n",
      "capacity\n",
      "already\n",
      "defined\n",
      "each\n",
      "link\n",
      "hence\n",
      "there\n",
      "define\n",
      "could\n",
      "generate\n",
      "quickly\n",
      "efficiently\n",
      "paper\n",
      "presenting\n",
      "new\n",
      "concept\n",
      "provide\n",
      "dynamic\n",
      "condition\n",
      "methods\n",
      "load\n",
      "links\n",
      "packet\n",
      "has\n",
      "minimized\n",
      "various\n",
      "evolutionary\n",
      "programming\n",
      "swarm\n",
      "comparative\n",
      "study\n",
      "given\n",
      "their\n",
      "terms\n",
      "converging\n",
      "speed\n",
      "universal\n",
      "approximation\n",
      "capability\n",
      "key\n",
      "feature\n",
      "feed\n",
      "forward\n",
      "applied\n",
      "predict\n",
      "total\n",
      "available\n",
      "present\n",
      "any\n",
      "variation\n",
      "can\n",
      "generated\n",
      "immediately\n",
      "minimum\n",
      "inclusion\n",
      "information\n",
      "will\n",
      "improved\n",
      "much\n",
      "long\n",
      "timescale\n",
      "credit\n",
      "assignment\n",
      "neuralnetworks\n",
      "external\n",
      "memory\n",
      "traditional\n",
      "recurrent\n",
      "networks\n",
      "usually\n",
      "involves\n",
      "back\n",
      "propagating\n",
      "through\n",
      "chain\n",
      "tied\n",
      "weight\n",
      "matrices\n",
      "length\n",
      "scales\n",
      "linearly\n",
      "steps\n",
      "run\n",
      "step\n",
      "creates\n",
      "many\n",
      "problems\n",
      "vanishing\n",
      "gradients\n",
      "have\n",
      "been\n",
      "well\n",
      "studied\n",
      "contrast\n",
      "nnem\n",
      "'\n",
      "s\n",
      "architecture\n",
      "activity\n",
      "doesn\n",
      "t\n",
      "involve\n",
      "though\n",
      "some\n",
      "architectures\n",
      "ntm\n",
      "do\n",
      "utilize\n",
      "controller\n",
      ").\n",
      "rather\n",
      "externally\n",
      "stored\n",
      "embedding\n",
      "vectors\n",
      "used\n",
      "but\n",
      "no\n",
      "messages\n",
      "passed\n",
      "previous\n",
      "means\n",
      "aren\n",
      "all\n",
      "necessary\n",
      "gradient\n",
      "short\n",
      "extremely\n",
      "numerous\n",
      "one\n",
      "per\n",
      "vector\n",
      "reused\n",
      "until\n",
      "leaves\n",
      "thus\n",
      "pass\n",
      "must\n",
      "entire\n",
      "duration\n",
      "problematic\n",
      "storage\n",
      "far\n",
      "surpasses\n",
      "actual\n",
      "memories\n",
      "extent\n",
      "large\n",
      "infeasible\n",
      "propagate\n",
      "high\n",
      "dimensional\n",
      "settings\n",
      "way\n",
      "around\n",
      "need\n",
      "hold\n",
      "onto\n",
      "recalculate\n",
      "whenever\n",
      "if\n",
      "observations\n",
      "too\n",
      "store\n",
      "domain\n",
      "interest\n",
      "direct\n",
      "reinstatement\n",
      "cannot\n",
      "occur\n",
      "instead\n",
      "rely\n",
      "learned\n",
      "autoencoder\n",
      "reinstate\n",
      "then\n",
      "since\n",
      "recalculated\n",
      "unlikely\n",
      "perfectly\n",
      "match\n",
      "try\n",
      "2\n",
      "approximations\n",
      "error\n",
      "w\n",
      "r\n",
      "transfer\n",
      "learning\n",
      "named\n",
      "entity\n",
      "recognition\n",
      "recent\n",
      "approaches\n",
      "anns\n",
      "shown\n",
      "promising\n",
      "results\n",
      "ner\n",
      "order\n",
      "achieve\n",
      "performances\n",
      "trained\n",
      "labeled\n",
      "dataset\n",
      "labels\n",
      "obtain\n",
      "user\n",
      "wants\n",
      "perform\n",
      "label\n",
      "scarcity\n",
      "particularly\n",
      "pronounced\n",
      "patient\n",
      "note\n",
      "de\n",
      "identification\n",
      "instance\n",
      "work\n",
      "analyze\n",
      "what\n",
      "may\n",
      "address\n",
      "issue\n",
      "demonstrate\n",
      "transferring\n",
      "ann\n",
      "model\n",
      "another\n",
      "limited\n",
      "improves\n",
      "upon\n",
      "state\n",
      "art\n",
      "two\n",
      "different\n",
      "datasets\n",
      "automated\n",
      "playtesting\n",
      "procedural\n",
      "personas\n",
      "mcts\n",
      "evolved\n",
      "heuristics\n",
      "describes\n",
      "player\n",
      "modeling\n",
      "its\n",
      "application\n",
      "automatic\n",
      "game\n",
      "content\n",
      "archetypal\n",
      "called\n",
      "theoretically\n",
      "grounded\n",
      "psychological\n",
      "decision\n",
      "theory\n",
      "implemented\n",
      "monte\n",
      "carlo\n",
      "tree\n",
      "search\n",
      "node\n",
      "selection\n",
      "criteria\n",
      "developed\n",
      "computation\n",
      "replacing\n",
      "standard\n",
      "ucb1\n",
      "criterion\n",
      "how\n",
      "varied\n",
      "corpus\n",
      "levels\n",
      "play\n",
      "styles\n",
      "enacted\n",
      "level\n",
      "artificially\n",
      "intelligent\n",
      "construct\n",
      "synthetic\n",
      "playtesters\n",
      "proposed\n",
      "tool\n",
      "when\n",
      "human\n",
      "readily\n",
      "or\n",
      "quick\n",
      "visualization\n",
      "potential\n",
      "interactions\n",
      "possible\n",
      "applications\n",
      "include\n",
      "interactive\n",
      "tools\n",
      "during\n",
      "development\n",
      "systems\n",
      "evaluations\n",
      "conducted\n",
      "within\n",
      "span\n",
      "silnet\n",
      "single\n",
      "multi\n",
      "view\n",
      "reconstruction\n",
      "silhouettes\n",
      "objective\n",
      "3d\n",
      "shape\n",
      "understanding\n",
      "multiple\n",
      "images\n",
      "end\n",
      "introduce\n",
      "loss\n",
      "handle\n",
      "views\n",
      "agnostic\n",
      "manner\n",
      "convolutional\n",
      "proxy\n",
      "task\n",
      "silhouette\n",
      "prediction\n",
      "directly\n",
      "mapping\n",
      "2d\n",
      "target\n",
      "most\n",
      "generalisation\n",
      "--\n",
      "example\n",
      "3\n",
      "4\n",
      "test\n",
      "synthetics\n",
      "blobby\n",
      "object\n",
      "useful\n",
      "pre\n",
      "challenging\n",
      "realistic\n",
      "sculpture\n",
      "indeed\n",
      "learnt\n",
      "finally\n",
      "exceeds\n",
      "shapenet\n",
      "benchmark\n",
      "proposal\n",
      "pattern\n",
      "sufficient\n",
      "principle\n",
      "cognitive\n",
      "science\n",
      "despite\n",
      "prevalence\n",
      "computational\n",
      "mind\n",
      "connectionist\n",
      "establishing\n",
      "principles\n",
      "still\n",
      "controversy\n",
      "inconclusive\n",
      "proposes\n",
      "general\n",
      "ambitious\n",
      "scientific\n",
      "formal\n",
      "physical\n",
      "definition\n",
      "also\n",
      "solve\n",
      "conceptual\n",
      "gaps\n",
      "field\n",
      "product\n",
      "classification\n",
      "e\n",
      "commerce\n",
      "distributional\n",
      "semantics\n",
      "automatically\n",
      "predicting\n",
      "taxonomy\n",
      "predefined\n",
      "hierarchy\n",
      "textual\n",
      "description\n",
      "title\n",
      "efficient\n",
      "require\n",
      "suitable\n",
      "representation\n",
      "document\n",
      "fast\n",
      "algorithms\n",
      "above\n",
      "challenges\n",
      "propose\n",
      "formation\n",
      "develop\n",
      "ensemble\n",
      "utilizing\n",
      "respect\n",
      "wise\n",
      "depth\n",
      "classifiers\n",
      "reduction\n",
      "final\n",
      "effectiveness\n",
      "data\n",
      "sets\n",
      "leading\n",
      "platform\n",
      "better\n",
      "evaluation\n",
      "metrics\n",
      "compared\n",
      "earlier\n",
      "towards\n",
      "quantitative\n",
      "measures\n",
      "ontology\n",
      "relatively\n",
      "aims\n",
      "apply\n",
      "theories\n",
      "disciplines\n",
      "philosophy\n",
      "linguistics\n",
      "logics\n",
      "specific\n",
      "tasks\n",
      "support\n",
      "research\n",
      "methodologies\n",
      "critically\n",
      "discuss\n",
      "question\n",
      "should\n",
      "evaluated\n",
      "would\n",
      "facilitate\n",
      "comparison\n",
      "recorded\n",
      "debating\n",
      "audio\n",
      "speeches\n",
      "first\n",
      "resource\n",
      "growing\n",
      "argumentation\n",
      "technologies\n",
      "detail\n",
      "process\n",
      "speech\n",
      "recording\n",
      "professional\n",
      "debaters\n",
      "transcription\n",
      "asr\n",
      "consequent\n",
      "processing\n",
      "text\n",
      "\"\n",
      "nlp\n",
      "friendly\n",
      "\",\n",
      "parallel\n",
      "manual\n",
      "gold\n",
      "reference\n",
      "transcripts\n",
      "release\n",
      "controversial\n",
      "topics\n",
      "5\n",
      "formats\n",
      "corresponding\n",
      "stages\n",
      "production\n",
      "intention\n",
      "allow\n",
      "purposes\n",
      "addition\n",
      "debate\n",
      "applying\n",
      "mining\n",
      "either\n",
      "noisy\n",
      "clean\n",
      "intend\n",
      "make\n",
      "further\n",
      "releases\n",
      "future\n",
      "online\n",
      "action\n",
      "incremental\n",
      "weighted\n",
      "covariance\n",
      "descriptors\n",
      "video\n",
      "segments\n",
      "recognize\n",
      "actions\n",
      "unsegmented\n",
      "streams\n",
      "continuous\n",
      "evidence\n",
      "accumulation\n",
      "predictions\n",
      "stream\n",
      "videos\n",
      "presents\n",
      "yet\n",
      "skeleton\n",
      "descriptor\n",
      "adopted\n",
      "accumulate\n",
      "updating\n",
      "takes\n",
      "following\n",
      "into\n",
      "consideration\n",
      "past\n",
      "frames\n",
      "less\n",
      "contribution\n",
      "informative\n",
      "contribute\n",
      "achieved\n",
      "simple\n",
      "nearest\n",
      "neighbor\n",
      "against\n",
      "set\n",
      "offline\n",
      "experimental\n",
      "msc\n",
      "12\n",
      "kinect\n",
      "gesture\n",
      "newly\n",
      "constructed\n",
      "efficacy\n",
      "exchanging\n",
      "owl\n",
      "ql\n",
      "knowledge\n",
      "bases\n",
      "base\n",
      "exchange\n",
      "important\n",
      "area\n",
      "interested\n",
      "between\n",
      "source\n",
      "connected\n",
      "fundamental\n",
      "mappings\n",
      "expressed\n",
      "profile\n",
      "logic\n",
      "dl\n",
      "lite_r\n",
      "consider\n",
      "computing\n",
      "solutions\n",
      "identified\n",
      "desirable\n",
      "translations\n",
      "materialized\n",
      "ucq\n",
      "representations\n",
      "optimally\n",
      "capture\n",
      "tbox\n",
      "extracted\n",
      "unions\n",
      "conjunctive\n",
      "queries\n",
      "former\n",
      "automata\n",
      "theoretic\n",
      "technique\n",
      "complexity\n",
      "range\n",
      "np\n",
      "exptime\n",
      "while\n",
      "nlogspace\n",
      "completeness\n",
      "segmental\n",
      "acoustic\n",
      "modelling\n",
      "connects\n",
      "conditional\n",
      "crf\n",
      "rnn\n",
      "extraction\n",
      "does\n",
      "features\n",
      "segmentation\n",
      "boundaries\n",
      "marginalises\n",
      "segmentations\n",
      "together\n",
      "essence\n",
      "self\n",
      "contained\n",
      "practical\n",
      "decoding\n",
      "issues\n",
      "up\n",
      "context\n",
      "timit\n",
      "17\n",
      "phone\n",
      "---\n",
      "best\n",
      "reported\n",
      "result\n",
      "crfs\n",
      "fact\n",
      "only\n",
      "zeroth\n",
      "without\n",
      "language\n",
      "comprehensive\n",
      "tutoring\n",
      "euphonic\n",
      "conjunctions\n",
      "sanskrit\n",
      "grammar\n",
      "sandhis\n",
      "form\n",
      "aspect\n",
      "morphology\n",
      "phonology\n",
      "modern\n",
      "studying\n",
      "about\n",
      "follow\n",
      "rigorous\n",
      "paninian\n",
      "embodied\n",
      "panini\n",
      "ashtadhyayi\n",
      "few\n",
      "sandhi\n",
      "rules\n",
      "examples\n",
      "beginners\n",
      "gain\n",
      "operation\n",
      "so\n",
      "exceptions\n",
      "complex\n",
      "precedence\n",
      "involved\n",
      "felt\n",
      "designed\n",
      "enable\n",
      "student\n",
      "learn\n",
      "relevant\n",
      "aphorisms\n",
      "evaluate\n",
      "progress\n",
      "forms\n",
      "basis\n",
      "multimedia\n",
      "tutor\n",
      "categories\n",
      "users\n",
      "including\n",
      "scholars\n",
      "extensive\n",
      "muse\n",
      "csp\n",
      "extension\n",
      "constraint\n",
      "satisfaction\n",
      "multiply\n",
      "segmented\n",
      "especially\n",
      "those\n",
      "segment\n",
      "partially\n",
      "shared\n",
      "variables\n",
      "arise\n",
      "naturally\n",
      "signal\n",
      "vision\n",
      "handwriting\n",
      "often\n",
      "low\n",
      "utilized\n",
      "compactly\n",
      "represent\n",
      "several\n",
      "instances\n",
      "common\n",
      "domains\n",
      "constraints\n",
      "they\n",
      "combined\n",
      "reducing\n",
      "required\n",
      "concepts\n",
      "consistency\n",
      "arc\n",
      "lexically\n",
      "ambiguous\n",
      "sentences\n",
      "sentence\n",
      "hypotheses\n",
      "parses\n",
      "syntactically\n",
      "correct\n",
      "provided\n",
      "create\n",
      "csps\n",
      "indicate\n",
      "variable\n",
      "spectral\n",
      "multivariate\n",
      "latent\n",
      "structure\n",
      "considers\n",
      "linear\n",
      "variety\n",
      "directed\n",
      "graphical\n",
      "discrete\n",
      "mixed\n",
      "gaussian\n",
      "hidden\n",
      "markov\n",
      "mixture\n",
      "trees\n",
      "setting\n",
      "samples\n",
      "certain\n",
      "goal\n",
      "estimate\n",
      "i\n",
      ".,\n",
      "graph\n",
      "underlying\n",
      "other\n",
      "recursive\n",
      "grouping\n",
      "bottom\n",
      "procedure\n",
      "recovering\n",
      "independent\n",
      "finite\n",
      "sample\n",
      "size\n",
      "bounds\n",
      "exact\n",
      "recovery\n",
      "reveal\n",
      "natural\n",
      "dependencies\n",
      "statistical\n",
      "structural\n",
      "properties\n",
      "joint\n",
      "furthermore\n",
      "guarantees\n",
      "explicit\n",
      "dependence\n",
      "dimensionality\n",
      "making\n",
      "applicable\n",
      "heart\n",
      "quartet\n",
      "determining\n",
      "relative\n",
      "topology\n",
      "second\n",
      "statistics\n",
      "camera\n",
      "calibration\n",
      "motion\n",
      "barcodes\n",
      "epipolar\n",
      "geometry\n",
      "cameras\n",
      "viewpoints\n",
      "matching\n",
      "points\n",
      "find\n",
      "cases\n",
      "objects\n",
      "scene\n",
      "suggesting\n",
      "point\n",
      "line\n",
      "correspondences\n",
      "orders\n",
      "magnitude\n",
      "increase\n",
      "robustness\n",
      "accuracy\n",
      "improvement\n",
      "signature\n",
      "barcode\n",
      "lines\n",
      "binary\n",
      "sequence\n",
      "indicating\n",
      "frame\n",
      "existence\n",
      "least\n",
      "foreground\n",
      "pixel\n",
      "having\n",
      "leads\n",
      "increased\n",
      "dependent\n",
      "increments\n",
      "organized\n",
      "groups\n",
      "commonalties\n",
      "exist\n",
      "amongst\n",
      "them\n",
      "ideal\n",
      "choice\n",
      "propositions\n",
      "atoms\n",
      "posterior\n",
      "borrow\n",
      "normalized\n",
      "prior\n",
      "derive\n",
      "appropriate\n",
      "exchangeable\n",
      "probability\n",
      "partition\n",
      "eppf\n",
      "),\n",
      "subsequently\n",
      "deduce\n",
      "inference\n",
      "likelihood\n",
      "derivation\n",
      "demonstration\n",
      "gaussians\n",
      "combination\n",
      "combinations\n",
      "crms\n",
      "superior\n",
      "inferred\n",
      "values\n",
      "mixing\n",
      "weights\n",
      "clusters\n",
      "both\n",
      "respond\n",
      "appropriately\n",
      "completely\n",
      "measure\n",
      "augmented\n",
      "wormhole\n",
      "connections\n",
      "empirical\n",
      "term\n",
      "dependency\n",
      "easily\n",
      "generalization\n",
      "vanilla\n",
      "suggest\n",
      "reduce\n",
      "effects\n",
      "creating\n",
      "shortcut\n",
      "tardis\n",
      "relation\n",
      "discovery\n",
      "sequences\n",
      "selective\n",
      "embeddings\n",
      "own\n",
      "states\n",
      "revisit\n",
      "needed\n",
      "acts\n",
      "effectively\n",
      "similarities\n",
      "turing\n",
      "machines\n",
      "d\n",
      "read\n",
      "write\n",
      "operations\n",
      "simpler\n",
      "addressing\n",
      "/\n",
      "substantially\n",
      "heuristic\n",
      "once\n",
      "becomes\n",
      "full\n",
      "makes\n",
      "type\n",
      "detailed\n",
      "analysis\n",
      "propagation\n",
      "manns\n",
      "report\n",
      "competitive\n",
      "assessment\n",
      "effect\n",
      "algorithmic\n",
      "schemes\n",
      "bayesian\n",
      "adopting\n",
      "bns\n",
      "complicated\n",
      "huge\n",
      "turned\n",
      "known\n",
      "characteristics\n",
      "never\n",
      "done\n",
      "before\n",
      "reason\n",
      "arts\n",
      "simulated\n",
      "considering\n",
      "rates\n",
      "noise\n",
      "investigate\n",
      "widespread\n",
      "scores\n",
      "pitfalls\n",
      "convergence\n",
      "allocating\n",
      "wireless\n",
      "channels\n",
      "decentralized\n",
      "channel\n",
      "quali\n",
      "ties\n",
      "varying\n",
      "unknown\n",
      "accessing\n",
      "reduced\n",
      "quality\n",
      "due\n",
      "interference\n",
      "inherent\n",
      "allocations\n",
      "maximize\n",
      "social\n",
      "welfare\n",
      "assuming\n",
      "adopt\n",
      "under\n",
      "conditions\n",
      "socially\n",
      "allocation\n",
      "achievable\n",
      "communications\n",
      "cooperation\n",
      "conclusion\n",
      "decreases\n",
      "uncertainty\n",
      "payoffs\n",
      "harder\n",
      "opti\n",
      "mal\n",
      "arco1\n",
      "belief\n",
      "oil\n",
      "market\n",
      "potentially\n",
      "class\n",
      "currently\n",
      "atlantic\n",
      "richfield\n",
      "company\n",
      "arco\n",
      "university\n",
      "southern\n",
      "california\n",
      "usc\n",
      "advanced\n",
      "implementation\n",
      "financial\n",
      "forecasting\n",
      "believed\n",
      "impact\n",
      "crude\n",
      "pictorial\n",
      "mac\n",
      "ii\n",
      "facilitates\n",
      "consensus\n",
      "among\n",
      "members\n",
      "team\n",
      "forecasts\n",
      "prices\n",
      "via\n",
      "analyses\n",
      "ability\n",
      "updated\n",
      "highlights\n",
      "flexibility\n",
      "musegan\n",
      "track\n",
      "sequential\n",
      "symbolic\n",
      "music\n",
      "accompaniment\n",
      "generating\n",
      "notable\n",
      "differences\n",
      "necessitating\n",
      "composed\n",
      "instruments\n",
      "tracks\n",
      "dynamics\n",
      "collectively\n",
      "unfold\n",
      "interdependently\n",
      "lastly\n",
      "musical\n",
      "notes\n",
      "grouped\n",
      "chords\n",
      "arpeggios\n",
      "melodies\n",
      "polyphonic\n",
      "thereby\n",
      "chronological\n",
      "ordering\n",
      "three\n",
      "gans\n",
      "differ\n",
      "assumptions\n",
      "accordingly\n",
      "referred\n",
      "jamming\n",
      "composer\n",
      "hybrid\n",
      "hundred\n",
      "thousand\n",
      "bars\n",
      "rock\n",
      "piano\n",
      "rolls\n",
      "five\n",
      "bass\n",
      "drums\n",
      "guitar\n",
      "strings\n",
      "intra\n",
      "inter\n",
      "subjective\n",
      "coherent\n",
      "four\n",
      "right\n",
      "scratch\n",
      "inputs\n",
      "extend\n",
      "ai\n",
      "cooperative\n",
      "accompany\n",
      "rendered\n",
      "https\n",
      "://\n",
      "salu133445\n",
      "github\n",
      "io\n",
      "person\n",
      "inverse\n",
      "incrementally\n",
      "goals\n",
      "wearer\n",
      "go\n",
      "seek\n",
      "trajectory\n",
      "darko\n",
      "goes\n",
      "semantic\n",
      "pick\n",
      "?),\n",
      "learns\n",
      "visual\n",
      "daily\n",
      "behaviors\n",
      "irl\n",
      "discovers\n",
      "rewards\n",
      "batch\n",
      "whereas\n",
      "transitions\n",
      "streaming\n",
      "competing\n",
      "empirically\n",
      "regret\n",
      "adobe\n",
      "mit\n",
      "submission\n",
      "dstc\n",
      "spoken\n",
      "pilot\n",
      "dialog\n",
      "tracking\n",
      "challenge\n",
      "focus\n",
      "consists\n",
      "tagging\n",
      "utterance\n",
      "slots\n",
      "obtains\n",
      "0\n",
      "52\n",
      "67\n",
      "f1\n",
      "act\n",
      "tourist\n",
      "guide\n",
      "respectively\n",
      "score\n",
      "prototypical\n",
      "shot\n",
      "omniglot\n",
      "$\n",
      "k\n",
      "$-\n",
      "building\n",
      "call\n",
      "map\n",
      "clustering\n",
      "part\n",
      "encoder\n",
      "output\n",
      "interpreted\n",
      "confidence\n",
      "region\n",
      "constructs\n",
      "direction\n",
      "distance\n",
      "metric\n",
      "uncertainties\n",
      "individual\n",
      "preferred\n",
      "equivalent\n",
      "parameters\n",
      "1\n",
      "20\n",
      "regime\n",
      "comparable\n",
      "explore\n",
      "down\n",
      "sampling\n",
      "fraction\n",
      "therefore\n",
      "hypothesize\n",
      "homogeneous\n",
      "noisier\n",
      "commonplace\n",
      "real\n",
      "world\n",
      "related\n",
      "family\n",
      "attribute\n",
      "covering\n",
      "situations\n",
      "attributes\n",
      "studies\n",
      "mechanisms\n",
      "constructing\n",
      "reducts\n",
      "consistent\n",
      "inconsistent\n",
      "families\n",
      "employ\n",
      "illustrate\n",
      "compute\n",
      "illustrates\n",
      "machine\n",
      "differential\n",
      "processes\n",
      "leverages\n",
      "advances\n",
      "probabilistic\n",
      "conservation\n",
      "laws\n",
      "parametric\n",
      "ordinary\n",
      "partial\n",
      "integro\n",
      "fractional\n",
      "operators\n",
      "priors\n",
      "modified\n",
      "according\n",
      "employed\n",
      "infer\n",
      "scarce\n",
      "possibly\n",
      "come\n",
      "black\n",
      "box\n",
      "mt\n",
      "cnn\n",
      "toxicity\n",
      "paramount\n",
      "importance\n",
      "health\n",
      "environmental\n",
      "protection\n",
      "existing\n",
      "built\n",
      "wide\n",
      "regressors\n",
      "dnn\n",
      "successful\n",
      "occasions\n",
      "like\n",
      "offers\n",
      "little\n",
      "elegance\n",
      "microscopic\n",
      "established\n",
      "charges\n",
      "surface\n",
      "areas\n",
      "free\n",
      "energies\n",
      "forest\n",
      "rf\n",
      "boosting\n",
      "gbdt\n",
      "equal\n",
      "footing\n",
      "non\n",
      "endpoints\n",
      "able\n",
      "outperform\n",
      "hyperspectral\n",
      "imaging\n",
      "sparse\n",
      "thesis\n",
      "spatio\n",
      "techniques\n",
      "image\n",
      "exposure\n",
      "reflectance\n",
      "perspective\n",
      "preservation\n",
      "found\n",
      "group\n",
      "lower\n",
      "higher\n",
      "subset\n",
      "bands\n",
      "databases\n",
      "made\n",
      "publicly\n",
      "compressed\n",
      "forensic\n",
      "sentiment\n",
      "literature\n",
      "survey\n",
      "day\n",
      "life\n",
      "always\n",
      "influenced\n",
      "people\n",
      "think\n",
      "ideas\n",
      "opinions\n",
      "others\n",
      "affected\n",
      "explosion\n",
      "web\n",
      "led\n",
      "podcasting\n",
      "blogging\n",
      "contributing\n",
      "rss\n",
      "bookmarking\n",
      "networking\n",
      "eruption\n",
      "mine\n",
      "vast\n",
      "resources\n",
      "opinion\n",
      "treatment\n",
      "sentiments\n",
      "subjectivity\n",
      "take\n",
      "look\n",
      "details\n",
      "supervised\n",
      "driven\n",
      "sa\n",
      "na\n",
      "ive\n",
      "byes\n",
      "maximum\n",
      "entropy\n",
      "svm\n",
      "voted\n",
      "perceptrons\n",
      "discussed\n",
      "strengths\n",
      "drawbacks\n",
      "touched\n",
      "see\n",
      "dimension\n",
      "analyzing\n",
      "psychology\n",
      "mainly\n",
      "janyce\n",
      "wiebe\n",
      "ways\n",
      "detect\n",
      "narrative\n",
      "discourse\n",
      "contemporary\n",
      "works\n",
      "rail\n",
      "risk\n",
      "averse\n",
      "imitation\n",
      "viable\n",
      "policies\n",
      "imitating\n",
      "expert\n",
      "behavior\n",
      "reward\n",
      "signals\n",
      "gail\n",
      "trajectories\n",
      "cost\n",
      "costs\n",
      "heavy\n",
      "tailed\n",
      "agents\n",
      "control\n",
      "tail\n",
      "events\n",
      "catastrophic\n",
      "failure\n",
      "likely\n",
      "encountered\n",
      "reliability\n",
      "questionable\n",
      "comes\n",
      "deployment\n",
      "sensitive\n",
      "robotic\n",
      "surgery\n",
      "autonomous\n",
      "driving\n",
      "aim\n",
      "occurrence\n",
      "minimizing\n",
      "quantify\n",
      "value\n",
      "cvar\n",
      "appears\n",
      "potent\n",
      "alternative\n",
      "fundamentals\n",
      "contributes\n",
      "randomized\n",
      "learner\n",
      "sc\n",
      "termed\n",
      "scns\n",
      "randomised\n",
      "layer\n",
      "slfnns\n",
      "randomly\n",
      "assign\n",
      "biases\n",
      "nodes\n",
      "light\n",
      "supervisory\n",
      "mechanism\n",
      "analytically\n",
      "constructive\n",
      "scn\n",
      "establish\n",
      "theoretical\n",
      "property\n",
      "versions\n",
      "presented\n",
      "regression\n",
      "simulation\n",
      "concerning\n",
      "remarkable\n",
      "merits\n",
      "intervention\n",
      "scope\n",
      "adaptation\n",
      "sound\n",
      "enhancement\n",
      "distributed\n",
      "quasi\n",
      "steady\n",
      "genetic\n",
      "scheme\n",
      "dga\n",
      "initial\n",
      "population\n",
      "divided\n",
      "classes\n",
      "female\n",
      "male\n",
      "cluster\n",
      "females\n",
      "reclustering\n",
      "produces\n",
      "separated\n",
      "locates\n",
      "position\n",
      "centroids\n",
      "plans\n",
      "co\n",
      "independently\n",
      "evolve\n",
      "separately\n",
      "merging\n",
      "place\n",
      "depending\n",
      "experimentation\n",
      "unimodal\n",
      "multimodal\n",
      "functions\n",
      "hierarchical\n",
      "electricity\n",
      "grid\n",
      "management\n",
      "power\n",
      "vital\n",
      "necessitates\n",
      "careful\n",
      "managing\n",
      "renewable\n",
      "generations\n",
      "demand\n",
      "unplanned\n",
      "outages\n",
      "face\n",
      "requires\n",
      "methodology\n",
      "tractable\n",
      "rl\n",
      "abstraction\n",
      "devise\n",
      "alternates\n",
      "slow\n",
      "scale\n",
      "policy\n",
      "prevailing\n",
      "strength\n",
      "pretraining\n",
      "dropout\n",
      "guiding\n",
      "unsupervised\n",
      "regularization\n",
      "parameter\n",
      "estimates\n",
      "denoising\n",
      "aspects\n",
      "mature\n",
      "interesting\n",
      "ask\n",
      "choices\n",
      "interaction\n",
      "backpropagation\n",
      "nonconvex\n",
      "objectives\n",
      "incorporate\n",
      "insights\n",
      "derived\n",
      "conduct\n",
      "compensation\n",
      "connectivity\n",
      "tau\n",
      "alzheimer\n",
      "disease\n",
      "updates\n",
      "simplistic\n",
      "ad\n",
      "behaviour\n",
      "synaptic\n",
      "compensatory\n",
      "calculating\n",
      "additionally\n",
      "introduces\n",
      "simulating\n",
      "neurofibrillary\n",
      "pathology\n",
      "resulting\n",
      "dramatic\n",
      "damage\n",
      "contrasting\n",
      "retrieval\n",
      "whilst\n",
      "obtained\n",
      "remote\n",
      "lead\n",
      "rapid\n",
      "q\n",
      "($\n",
      "λ\n",
      "$)\n",
      "off\n",
      "corrections\n",
      "alternate\n",
      "difference\n",
      "returns\n",
      "corrected\n",
      "current\n",
      "transition\n",
      "probabilities\n",
      "prove\n",
      "approximate\n",
      "relate\n",
      "eligibility\n",
      "trace\n",
      "discount\n",
      "factor\n",
      "formalize\n",
      "tradeoff\n",
      "td\n",
      "($\\\n",
      "lambda\n",
      "$).\n",
      "relationship\n",
      "acquisition\n",
      "database\n",
      "samsung\n",
      "superconductor\n",
      "facility\n",
      "fulfill\n",
      "kstar\n",
      "korea\n",
      "superconducting\n",
      "tokamak\n",
      "magnet\n",
      "conductor\n",
      "sstf\n",
      "institute\n",
      "technology\n",
      "dac\n",
      "unix\n",
      "vxworks\n",
      "os\n",
      "vme\n",
      "epics\n",
      "physics\n",
      "industrial\n",
      "communication\n",
      "ioc\n",
      "server\n",
      "client\n",
      "program\n",
      "linux\n",
      "workstation\n",
      "pentium\n",
      "status\n",
      "summarization\n",
      "compression\n",
      "anaphoricity\n",
      "discriminative\n",
      "integrally\n",
      "combines\n",
      "selects\n",
      "units\n",
      "summary\n",
      "rich\n",
      "whose\n",
      "deletion\n",
      "licensed\n",
      "subsentential\n",
      "cross\n",
      "coherence\n",
      "guaranteeing\n",
      "pronoun\n",
      "included\n",
      "antecedent\n",
      "rewritten\n",
      "mention\n",
      "outperforms\n",
      "rouge\n",
      "judgments\n",
      "linguistic\n",
      "generalized\n",
      "*\n",
      "lightest\n",
      "global\n",
      "formulated\n",
      "generalize\n",
      "abstractions\n",
      "broad\n",
      "describe\n",
      "searches\n",
      "derivations\n",
      "gives\n",
      "searching\n",
      "graphs\n",
      "fashion\n",
      "described\n",
      "pipeline\n",
      "passing\n",
      "forth\n",
      "perceptual\n",
      "estimating\n",
      "convex\n",
      "grayscale\n",
      "compositional\n",
      "finding\n",
      "salient\n",
      "curves\n",
      "aggregation\n",
      "justifiable\n",
      "granularity\n",
      "combine\n",
      "numeric\n",
      "membership\n",
      "combining\n",
      "interval\n",
      "associated\n",
      "meta\n",
      "granules\n",
      "diversity\n",
      "findings\n",
      "produced\n",
      "quantified\n",
      "intervals\n",
      "select\n",
      "ten\n",
      "fifteen\n",
      "build\n",
      "heterogeneous\n",
      "experiment\n",
      "uci\n",
      "performs\n",
      "six\n",
      "trainable\n",
      "adaboost\n",
      "bagging\n",
      "subspace\n",
      "investigation\n",
      "color\n",
      "constancy\n",
      "ubiquitous\n",
      "lan\n",
      "positioning\n",
      "outcome\n",
      "hybridization\n",
      "mobile\n",
      "typical\n",
      "analyzed\n",
      "location\n",
      "environment\n",
      "fitting\n",
      "absolute\n",
      "conventional\n",
      "expected\n",
      "preliminary\n",
      "indoor\n",
      "types\n",
      "phrases\n",
      "languages\n",
      "calculated\n",
      "modeled\n",
      "advance\n",
      "sum\n",
      "valid\n",
      "backward\n",
      "computations\n",
      "resorting\n",
      "meaningful\n",
      "respective\n",
      "contexts\n",
      "portuguese\n",
      "word\n",
      "evaluating\n",
      "analogies\n",
      "words\n",
      "become\n",
      "sys\n",
      "tems\n",
      "brazilian\n",
      "european\n",
      "variants\n",
      "31\n",
      "fasttext\n",
      "glove\n",
      "wang2vec\n",
      "word2vec\n",
      "intrinsically\n",
      "syntactic\n",
      "extrinsically\n",
      "pos\n",
      "similarity\n",
      "appear\n",
      "option\n",
      "regularized\n",
      "exhibit\n",
      "excellent\n",
      "utility\n",
      "across\n",
      "fine\n",
      "tuning\n",
      "simplest\n",
      "widely\n",
      "seeks\n",
      "exploit\n",
      "adapt\n",
      "itis\n",
      "sub\n",
      "prevent\n",
      "severe\n",
      "optimality\n",
      "overfitting\n",
      "ensuring\n",
      "regularizing\n",
      "identify\n",
      "neurons\n",
      "shares\n",
      "activation\n",
      "patterns\n",
      "tuned\n",
      "neuron\n",
      "belong\n",
      "trivial\n",
      "activations\n",
      "depend\n",
      "groupings\n",
      "conditioned\n",
      "assignments\n",
      "popular\n",
      "strategies\n",
      "than10\n",
      "developments\n",
      "shortest\n",
      "distances\n",
      "lately\n",
      "suggestions\n",
      "parametrized\n",
      "commute\n",
      "resistance\n",
      "developing\n",
      "risen\n",
      "mentioned\n",
      "fail\n",
      "account\n",
      "article\n",
      "dissimilarity\n",
      "foundation\n",
      "computed\n",
      "closed\n",
      "pairs\n",
      "moreover\n",
      "seen\n",
      "upgrade\n",
      "defines\n",
      "satisfies\n",
      "geodetic\n",
      "straightforward\n",
      "interpolate\n",
      "focuses\n",
      "applicability\n",
      "prosper\n",
      "tlp\n",
      "50\n",
      "scenarios\n",
      "encompassing\n",
      "400\n",
      "minutes\n",
      "676k\n",
      "folds\n",
      "larger\n",
      "8\n",
      "covered\n",
      "generic\n",
      "paves\n",
      "suitably\n",
      "train\n",
      "avoiding\n",
      "reflect\n",
      "trackers\n",
      "rank\n",
      "speeds\n",
      "thorough\n",
      "qualitative\n",
      "highlighting\n",
      "benchmarks\n",
      "bring\n",
      "widen\n",
      "b\n",
      "abruptly\n",
      "drops\n",
      "efforts\n",
      "multiagent\n",
      "bidirectionally\n",
      "coordinated\n",
      "nets\n",
      "emergence\n",
      "coordination\n",
      "starcraft\n",
      "combat\n",
      "games\n",
      "collaborative\n",
      "effort\n",
      "agent\n",
      "indispensable\n",
      "case\n",
      "coordinate\n",
      "defeat\n",
      "enemies\n",
      "maintain\n",
      "scalable\n",
      "bicnet\n",
      "['\n",
      "biknet\n",
      "])\n",
      "vectorised\n",
      "actor\n",
      "critic\n",
      "formulation\n",
      "combats\n",
      "arbitrary\n",
      "numbers\n",
      "sides\n",
      "demonstrates\n",
      "supervisions\n",
      "demonstrations\n",
      "labelled\n",
      "commonly\n",
      "experienced\n",
      "players\n",
      "baselines\n",
      "possesses\n",
      "optimize\n",
      "recently\n",
      "shallow\n",
      "suited\n",
      "consistently\n",
      "robust\n",
      "stochasticity\n",
      "net\n",
      "mnist\n",
      "generalizes\n",
      "toronto\n",
      "faces\n",
      "cifar\n",
      "10\n",
      "100\n",
      "albeit\n",
      "existed\n",
      "became\n",
      "starting\n",
      "nineties\n",
      "systematic\n",
      "analyse\n",
      "put\n",
      "increasing\n",
      "allows\n",
      "nowadays\n",
      "sophisticated\n",
      "ant\n",
      "colony\n",
      "optimisation\n",
      "immune\n",
      "combinatorial\n",
      "chapter\n",
      "mathematical\n",
      "runtime\n",
      "attention\n",
      "fitness\n",
      "drift\n",
      "aid\n",
      "reader\n",
      "comprehension\n",
      "concluded\n",
      "providing\n",
      "references\n",
      "extensions\n",
      "obtainment\n",
      "paradigm\n",
      "assistive\n",
      "home\n",
      "design\n",
      "elderly\n",
      "special\n",
      "needs\n",
      "suffering\n",
      "disabilities\n",
      "chronic\n",
      "diseases\n",
      "possibility\n",
      "retaining\n",
      "independence\n",
      "society\n",
      "assistance\n",
      "care\n",
      "being\n",
      "rapidly\n",
      "almost\n",
      "industrialized\n",
      "emerging\n",
      "countries\n",
      "paradigms\n",
      "ensure\n",
      "activities\n",
      "living\n",
      "correctly\n",
      "regularly\n",
      "assisted\n",
      "perception\n",
      "engine\n",
      "core\n",
      "arianna\n",
      "understand\n",
      "whether\n",
      "adl\n",
      "motivate\n",
      "him\n",
      "her\n",
      "performing\n",
      "mediated\n",
      "motivational\n",
      "dialogue\n",
      "nearables\n",
      "installed\n",
      "apartment\n",
      "plus\n",
      "wearable\n",
      "worn\n",
      "fit\n",
      "garments\n",
      "remains\n",
      "receives\n",
      "adapts\n",
      "getting\n",
      "clear\n",
      "subtle\n",
      "experience\n",
      "designing\n",
      "argue\n",
      "coupling\n",
      "validation\n",
      "centered\n",
      "analytics\n",
      "guided\n",
      "exemplar\n",
      "complement\n",
      "role\n",
      "opportunities\n",
      "visible\n",
      "roles\n",
      "humans\n",
      "parameterized\n",
      "return\n",
      "update\n",
      "gac\n",
      "firstly\n",
      "locally\n",
      "maximizes\n",
      "main\n",
      "contributions\n",
      "curvature\n",
      "hessians\n",
      "deterministic\n",
      "ignored\n",
      "controls\n",
      "vehicles\n",
      "bird\n",
      "eye\n",
      "awareness\n",
      "road\n",
      "essential\n",
      "component\n",
      "driver\n",
      "gaining\n",
      "academia\n",
      "car\n",
      "companies\n",
      "aware\n",
      "transformation\n",
      "maps\n",
      "detections\n",
      "dashboard\n",
      "broader\n",
      "occupancy\n",
      "featuring\n",
      "1m\n",
      "couples\n",
      "taken\n",
      "collected\n",
      "annotated\n",
      "warp\n",
      "solely\n",
      "ones\n",
      "retail\n",
      "practices\n",
      "abms\n",
      "working\n",
      "productivity\n",
      "offer\n",
      "organizational\n",
      "capabilities\n",
      "disciplinary\n",
      "worked\n",
      "uk\n",
      "department\n",
      "collect\n",
      "perceptions\n",
      "actors\n",
      "departments\n",
      "simulator\n",
      "gather\n",
      "regarding\n",
      "empowerment\n",
      "employee\n",
      "improvised\n",
      "comedy\n",
      "improvisational\n",
      "theatre\n",
      "partner\n",
      "skill\n",
      "talented\n",
      "proficient\n",
      "shine\n",
      "\".\n",
      "live\n",
      "alongside\n",
      "audiences\n",
      "course\n",
      "30\n",
      "audience\n",
      "3000\n",
      "refined\n",
      "theatrical\n",
      "times\n",
      "structures\n",
      "participants\n",
      "complete\n",
      "submitted\n",
      "suspension\n",
      "disbelief\n",
      "believe\n",
      "creation\n",
      "segmenting\n",
      "assumes\n",
      "written\n",
      "uniform\n",
      "background\n",
      "assumption\n",
      "character\n",
      "contain\n",
      "graphics\n",
      "symbols\n",
      "actually\n",
      "easy\n",
      "backgrounds\n",
      "holes\n",
      "containing\n",
      "candidates\n",
      "candidate\n",
      "tested\n",
      "presence\n",
      "hull\n",
      "65\n",
      "english\n",
      "urdu\n",
      "63\n",
      "filter\n",
      "fingerprint\n",
      "plays\n",
      "commercial\n",
      "millions\n",
      "every\n",
      "g\n",
      "unlocking\n",
      "phones\n",
      "typically\n",
      "divides\n",
      "negative\n",
      "true\n",
      "minutiae\n",
      "lost\n",
      "conversely\n",
      "misclassified\n",
      "spurious\n",
      "introduced\n",
      "threefold\n",
      "factorized\n",
      "directional\n",
      "bandpass\n",
      "fdb\n",
      "texture\n",
      "hilbert\n",
      "transform\n",
      "butterworth\n",
      "dhbb\n",
      "interwoven\n",
      "soft\n",
      "thresholding\n",
      "secondly\n",
      "manually\n",
      "marked\n",
      "ground\n",
      "truth\n",
      "10560\n",
      "thirdly\n",
      "cited\n",
      "showing\n",
      "clearly\n",
      "parsing\n",
      "reranking\n",
      "re\n",
      "parser\n",
      "scorer\n",
      "feasible\n",
      "constraining\n",
      "bidirectional\n",
      "catches\n",
      "besides\n",
      "substantial\n",
      "stand\n",
      "implement\n",
      "eager\n",
      "stack\n",
      "lstms\n",
      "oracle\n",
      "supporting\n",
      "uas\n",
      "66\n",
      "chinese\n",
      "las\n",
      "32\n",
      "lass\n",
      "achieving\n",
      "87\n",
      "58\n",
      "93\n",
      "37\n",
      "stereo\n",
      "dense\n",
      "optical\n",
      "ego\n",
      "moving\n",
      "rig\n",
      "rigid\n",
      "disparity\n",
      "dof\n",
      "odometry\n",
      "regions\n",
      "estimated\n",
      "fused\n",
      "fusion\n",
      "moves\n",
      "unified\n",
      "benefits\n",
      "overall\n",
      "efficiency\n",
      "ranked\n",
      "third\n",
      "kitti\n",
      "2015\n",
      "runs\n",
      "seconds\n",
      "top\n",
      "sintel\n",
      "osf\n",
      "[\n",
      "menze\n",
      "geiger\n",
      "],\n",
      "bounding\n",
      "annotations\n",
      "cnns\n",
      "great\n",
      "locations\n",
      "local\n",
      "enhance\n",
      "extract\n",
      "proposals\n",
      "treated\n",
      "bag\n",
      "extracting\n",
      "strong\n",
      "add\n",
      "relationships\n",
      "utilizes\n",
      "weak\n",
      "importantly\n",
      "boost\n",
      "extensively\n",
      "hand\n",
      "crafted\n",
      "validate\n",
      "minibatches\n",
      "minibatching\n",
      "highly\n",
      "practitioners\n",
      "accelerate\n",
      "utilization\n",
      "variance\n",
      "preferential\n",
      "capable\n",
      "accelerating\n",
      "considerable\n",
      "community\n",
      "technical\n",
      "difficulty\n",
      "{\\\n",
      "em\n",
      "}\n",
      "give\n",
      "reach\n",
      "classifieurs\n",
      "pour\n",
      "la\n",
      "sonar\n",
      "classifier\n",
      "sea\n",
      "bed\n",
      "characterization\n",
      "classi\n",
      "fication\n",
      "sediment\n",
      "polish\n",
      "wordnets\n",
      "interconnected\n",
      "nouns\n",
      "verbs\n",
      "adjectives\n",
      "adverbs\n",
      "relations\n",
      "determine\n",
      "interlingual\n",
      "deliberate\n",
      "lexicon\n",
      "inferring\n",
      "biological\n",
      "integration\n",
      "gene\n",
      "expression\n",
      "characterizing\n",
      "genes\n",
      "products\n",
      "spite\n",
      "genomes\n",
      "organisms\n",
      "sequenced\n",
      "experimentally\n",
      "unfeasible\n",
      "previously\n",
      "premise\n",
      "guilt\n",
      "association\n",
      "advantage\n",
      "clustered\n",
      "biologists\n",
      "landmark\n",
      "identifying\n",
      "interface\n",
      "http\n",
      "fich\n",
      "unl\n",
      "edu\n",
      "ar\n",
      "sinc\n",
      "webdemo\n",
      "gamma\n",
      "am\n",
      "/.\n",
      "toward\n",
      "display\n",
      "years\n",
      "gestures\n",
      "explored\n",
      "hci\n",
      "structured\n",
      "observable\n",
      "kinematical\n",
      "primitives\n",
      "pertinent\n",
      "verbal\n",
      "distinguishes\n",
      "deixis\n",
      "testbed\n",
      "imap\n",
      "enabled\n",
      "elicitation\n",
      "adequate\n",
      "mode\n",
      "significance\n",
      "accounting\n",
      "alignment\n",
      "parts\n",
      "suggests\n",
      "organization\n",
      "lexical\n",
      "operator\n",
      "elimination\n",
      "toyelim\n",
      "prototype\n",
      "functionality\n",
      "propositional\n",
      "boolean\n",
      "quantifiers\n",
      "projection\n",
      "forgetting\n",
      "circumscription\n",
      "express\n",
      "representational\n",
      "abductive\n",
      "explanations\n",
      "operational\n",
      "backed\n",
      "genaue\n",
      "modellbasierte\n",
      "identifikation\n",
      "von\n",
      "gynäkologischen\n",
      "katheterpfaden\n",
      "für\n",
      "die\n",
      "mrt\n",
      "bildgestützte\n",
      "brachytherapie\n",
      "german\n",
      "abstract\n",
      "mortality\n",
      "gynecologic\n",
      "cancers\n",
      "cervical\n",
      "ovarian\n",
      "vaginal\n",
      "vulvar\n",
      "internationally\n",
      "].\n",
      "radiotherapy\n",
      "supplemented\n",
      "brachytherapy\n",
      "administered\n",
      "doses\n",
      "magnetic\n",
      "resonance\n",
      "mri\n",
      "differentiate\n",
      "tissue\n",
      "intraoperative\n",
      "planning\n",
      "tomography\n",
      "ct\n",
      "dark\n",
      "diffuse\n",
      "appearance\n",
      "catheter\n",
      "addressed\n",
      "precise\n",
      "herein\n",
      "tracing\n",
      "catheters\n",
      "tip\n",
      "pc\n",
      "precision\n",
      "indicators\n",
      "clinical\n",
      "gynecological\n",
      "after\n",
      "op\n",
      "healthy\n",
      "surrounding\n",
      "tumor\n",
      "irradiated\n",
      "reduces\n",
      "leaving\n",
      "behind\n",
      "residual\n",
      "cells\n",
      "cause\n",
      "recurrence\n",
      "cancer\n",
      "metastases\n",
      "secondary\n",
      "tumors\n",
      "elsewhere\n",
      "body\n",
      "cervix\n",
      "prostate\n",
      "minimally\n",
      "invasive\n",
      "ie\n",
      "removal\n",
      "irradiation\n",
      "keyhole\n",
      "open\n",
      "active\n",
      "accurate\n",
      "estimation\n",
      "uniformly\n",
      "budget\n",
      "sampled\n",
      "maker\n",
      "query\n",
      "incoming\n",
      "corrupted\n",
      "ucb\n",
      "balancing\n",
      "expectation\n",
      "contextual\n",
      "remarkably\n",
      "outperforming\n",
      "violated\n",
      "deblocking\n",
      "blocking\n",
      "artifact\n",
      "frequently\n",
      "coded\n",
      "bit\n",
      "visually\n",
      "annoying\n",
      "hurts\n",
      "viewed\n",
      "superimposition\n",
      "intrinsic\n",
      "layers\n",
      "seems\n",
      "severely\n",
      "ill\n",
      "posed\n",
      "unknowns\n",
      "recover\n",
      "twice\n",
      "measurements\n",
      "separate\n",
      "exploits\n",
      "sparsity\n",
      "fields\n",
      "lagrangian\n",
      "multiplier\n",
      "simplicity\n",
      "mutual\n",
      "agglomerative\n",
      "ahc\n",
      "raises\n",
      "correction\n",
      "formulate\n",
      "normal\n",
      "shrinks\n",
      "priori\n",
      "identity\n",
      "provides\n",
      "stopping\n",
      "rule\n",
      "corrects\n",
      "log\n",
      "bayes\n",
      "asymptotically\n",
      "proportional\n",
      "plug\n",
      "additive\n",
      "agreement\n",
      "investigated\n",
      "alternatives\n",
      "asymptotic\n",
      "encouraging\n",
      "outperformed\n",
      "shelf\n",
      "raw\n",
      "toy\n",
      "functional\n",
      "fmri\n",
      "measuring\n",
      "brain\n",
      "procedures\n",
      "had\n",
      "atypical\n",
      "sense\n",
      "systematically\n",
      "favored\n",
      "detection\n",
      "emotion\n",
      "recognizing\n",
      "themes\n",
      "conducts\n",
      "emotions\n",
      "considered\n",
      "neutral\n",
      "anger\n",
      "joy\n",
      "sadness\n",
      "distinuishability\n",
      "emotional\n",
      "followed\n",
      "custom\n",
      "prepared\n",
      "peak\n",
      "tests\n",
      "formed\n",
      "subjects\n",
      "infinity\n",
      "computable\n",
      "contrary\n",
      "supposition\n",
      "guaranteed\n",
      "infinite\n",
      "extends\n",
      "implications\n",
      "consisting\n",
      "fidelity\n",
      "penalty\n",
      "strictly\n",
      "nuclear\n",
      "norm\n",
      "$\\\n",
      "ell_1\n",
      "admm\n",
      "guarantee\n",
      "scalar\n",
      "adjacency\n",
      "representing\n",
      "protein\n",
      "`\n",
      "escherichia\n",
      "coli\n",
      "bacteria\n",
      "differentiation\n",
      "freely\n",
      "quite\n",
      "abundance\n",
      "worthy\n",
      "want\n",
      "possibilities\n",
      "arising\n",
      "calculus\n",
      "pure\n",
      "mathematics\n",
      "1920\n",
      "started\n",
      "shall\n",
      "calculation\n",
      "edge\n",
      "restoration\n",
      "faint\n",
      "astronomical\n",
      "coding\n",
      "vs\n",
      "bottleneck\n",
      "autoencoders\n",
      "actively\n",
      "researched\n",
      "subjectively\n",
      "reconstructed\n",
      "degree\n",
      "yields\n",
      "psnr\n",
      "ssim\n",
      "correlate\n",
      "18\n",
      "06\n",
      "\\%\n",
      "7\n",
      "although\n",
      "intensive\n",
      "otherwise\n",
      "circle\n",
      "curve\n",
      "introductory\n",
      "maths\n",
      "textbooks\n",
      "discusses\n",
      "elementary\n",
      "geometric\n",
      "technological\n",
      "geogebra\n",
      "refute\n",
      "false\n",
      "impression\n",
      "conjecture\n",
      "confirm\n",
      "proving\n",
      "rigorously\n",
      "classrooms\n",
      "middle\n",
      "school\n",
      "cheaper\n",
      "next\n",
      "sequencing\n",
      "unprecedentedly\n",
      "massive\n",
      "genomic\n",
      "epigenomic\n",
      "near\n",
      "routine\n",
      "medical\n",
      "record\n",
      "wellness\n",
      "validity\n",
      "significantly\n",
      "usefulness\n",
      "diagnosis\n",
      "prognosis\n",
      "predictive\n",
      "phenotype\n",
      "hypothesis\n",
      "project\n",
      "original\n",
      "preserving\n",
      "response\n",
      "phenotypes\n",
      "clinically\n",
      "significant\n",
      "variant\n",
      "sdr\n",
      "predictors\n",
      "dividing\n",
      "whole\n",
      "genome\n",
      "subsdr\n",
      "turn\n",
      "scoring\n",
      "remove\n",
      "row\n",
      "alternating\n",
      "multipliers\n",
      "nhlbi\n",
      "exome\n",
      "timeml\n",
      "compliant\n",
      "turkish\n",
      "acknowledged\n",
      "extractors\n",
      "components\n",
      "answering\n",
      "normalization\n",
      "expressions\n",
      "except\n",
      "date\n",
      "event\n",
      "annotation\n",
      "texts\n",
      "created\n",
      "devised\n",
      "considerably\n",
      "coverage\n",
      "blocks\n",
      "convenient\n",
      "prospective\n",
      "ancient\n",
      "branching\n",
      "ineffective\n",
      "hardware\n",
      "change\n",
      "drastically\n",
      "stone\n",
      "maddison\n",
      "et\n",
      "al\n",
      ");\n",
      "clark\n",
      "&\n",
      "storkey\n",
      ")]\n",
      "dcnn\n",
      "predicts\n",
      "move\n",
      ")-\n",
      "engines\n",
      "pachi\n",
      "baudis\n",
      "gailly\n",
      "2012\n",
      "idea\n",
      "bot\n",
      "darkforest\n",
      "relies\n",
      "win\n",
      "looser\n",
      "budgets\n",
      "newest\n",
      "darkfores2\n",
      "stable\n",
      "kgs\n",
      "4k\n",
      "5k\n",
      "ranks\n",
      "adding\n",
      "stronger\n",
      "darkfmcts3\n",
      "5000\n",
      "rollouts\n",
      "beats\n",
      "10k\n",
      "250\n",
      "75k\n",
      "achieves\n",
      "5d\n",
      "par\n",
      "ais\n",
      "zen\n",
      "dolbaram\n",
      "crazystone\n",
      "alphago\n",
      "silver\n",
      "2016\n",
      ")];\n",
      "110k\n",
      "won\n",
      "3rd\n",
      "january\n",
      "tournament\n",
      "inertial\n",
      "aided\n",
      "rolling\n",
      "shutter\n",
      "pose\n",
      "decades\n",
      "captures\n",
      "gravity\n",
      "angular\n",
      "velocity\n",
      "rotation\n",
      "instantaneous\n",
      "causes\n",
      "distortion\n",
      "simplify\n",
      "unlike\n",
      "44\n",
      "9\n",
      "11\n",
      "public\n",
      "penncosyvio\n",
      "evolvability\n",
      "inevitable\n",
      "pressure\n",
      "why\n",
      "unresolved\n",
      "insight\n",
      "heritable\n",
      "unbiased\n",
      "drifting\n",
      "genotypes\n",
      "biased\n",
      "evolvable\n",
      "phenotypic\n",
      "divergence\n",
      "correlates\n",
      "founding\n",
      "niches\n",
      "niche\n",
      "founders\n",
      "growth\n",
      "genotypic\n",
      "bias\n",
      "interestingly\n",
      "compete\n",
      "series\n",
      "inevitably\n",
      "passive\n",
      "tendency\n",
      "chat\n",
      "generates\n",
      "responses\n",
      "conversation\n",
      "comprising\n",
      "topic\n",
      "theme\n",
      "dom\n",
      "seq2seq\n",
      "targeted\n",
      "sutskever\n",
      "2014\n",
      "utterances\n",
      "completion\n",
      "factors\n",
      "regular\n",
      "records\n",
      "monitor\n",
      "incomplete\n",
      "unpunctuality\n",
      "absence\n",
      "patients\n",
      "resolve\n",
      "missing\n",
      "tensor\n",
      "suggested\n",
      "imputation\n",
      "papers\n",
      "correlated\n",
      "correlation\n",
      "along\n",
      "decompose\n",
      "imposed\n",
      "hospital\n",
      "comparing\n",
      "nuances\n",
      "personality\n",
      "frequency\n",
      "pronouns\n",
      "tense\n",
      "correlations\n",
      "traits\n",
      "affect\n",
      "excitement\n",
      "yearning\n",
      "admiration\n",
      "gains\n",
      "coarse\n",
      "specificity\n",
      "detecting\n",
      "essays\n",
      "statistically\n",
      "baseline\n",
      "corrupt\n",
      "bandits\n",
      "privacy\n",
      "armed\n",
      "bandit\n",
      "mab\n",
      "motivated\n",
      "recommender\n",
      "unobserved\n",
      "corruption\n",
      "bound\n",
      "frequentist\n",
      "klucb\n",
      "cf\n",
      "ts\n",
      "upper\n",
      "desired\n",
      "impacts\n",
      "cohomology\n",
      "digital\n",
      "ring\n",
      "valued\n",
      "pictures\n",
      "picture\n",
      "$,\n",
      "simplicial\n",
      "topologically\n",
      "isomorphisms\n",
      "visualizing\n",
      "topological\n",
      "thinning\n",
      "representative\n",
      "cycles\n",
      "homology\n",
      "generators\n",
      "cup\n",
      "food\n",
      "reviews\n",
      "carefully\n",
      "gru\n",
      "deal\n",
      "skewed\n",
      "accuracies\n",
      "morphological\n",
      "derivational\n",
      "readable\n",
      "dictionary\n",
      "aiming\n",
      "emerge\n",
      "regularities\n",
      "contains\n",
      "connect\n",
      "headword\n",
      "morphologically\n",
      "french\n",
      "tlfi\n",
      "authentication\n",
      "smartphones\n",
      "verification\n",
      "maryland\n",
      "02\n",
      "umdaa\n",
      "modal\n",
      "sensors\n",
      "front\n",
      "touch\n",
      "sensor\n",
      "service\n",
      "modalities\n",
      "satisfactory\n",
      "promoting\n",
      "business\n",
      "news\n",
      "explains\n",
      "abnormal\n",
      "volatility\n",
      "flows\n",
      "today\n",
      "cornerstones\n",
      "sciences\n",
      "economics\n",
      "regard\n",
      "quantifying\n",
      "geopolitical\n",
      "economic\n",
      "etc\n",
      ".)\n",
      "trading\n",
      "pricing\n",
      "firms\n",
      "stock\n",
      "markets\n",
      "24\n",
      "million\n",
      "thompson\n",
      "reuters\n",
      "206\n",
      "major\n",
      "stocks\n",
      "p\n",
      "us\n",
      "index\n",
      "price\n",
      "movements\n",
      "summarized\n",
      "regressions\n",
      "pieces\n",
      "decomposed\n",
      "thematic\n",
      "basket\n",
      "examination\n",
      "distributions\n",
      "confirms\n",
      "influencing\n",
      "puzzling\n",
      "stylized\n",
      "economies\n",
      "namely\n",
      "volumes\n",
      "abnormally\n",
      ",\"\n",
      "explained\n",
      "excess\n",
      "restricting\n",
      "genuinely\n",
      "retinal\n",
      "vessel\n",
      "histogram\n",
      "morlet\n",
      "wavelet\n",
      "blood\n",
      "vessels\n",
      "preprocessing\n",
      "equalization\n",
      "pixels\n",
      "classified\n",
      "continues\n",
      "noises\n",
      "gmm\n",
      "approximated\n",
      "specialist\n",
      "transforms\n",
      "directions\n",
      "discontinuities\n",
      "drive\n",
      "9571\n",
      "enables\n",
      "checking\n",
      "yield\n",
      "dealt\n",
      "uncertain\n",
      "bv\n",
      "definable\n",
      "errors\n",
      "palmprint\n",
      "biometrics\n",
      "isomorphic\n",
      "medoids\n",
      "partitioning\n",
      "uses\n",
      "pam\n",
      "n\n",
      "invariant\n",
      "sift\n",
      "drawn\n",
      "probable\n",
      "pair\n",
      "searched\n",
      "iterative\n",
      "relaxation\n",
      "pairing\n",
      "keypoint\n",
      "concatenation\n",
      "utmost\n",
      "deepnorm\n",
      "sproat\n",
      "jaitly\n",
      "aligned\n",
      "token\n",
      "normalizing\n",
      "tricky\n",
      "google\n",
      "days\n",
      "gpu\n",
      "97\n",
      "62\n",
      "impressive\n",
      "worlds\n",
      "translation\n",
      "simultaneous\n",
      "difficulties\n",
      "molecules\n",
      "cryo\n",
      "electron\n",
      "microscopy\n",
      "orientations\n",
      "heterogeneity\n",
      "conformations\n",
      "molecule\n",
      "prone\n",
      "reaching\n",
      "suboptimal\n",
      "minima\n",
      "vice\n",
      "versa\n",
      "compact\n",
      "relaxations\n",
      "semidefinite\n",
      "programs\n",
      "sdps\n",
      "certificates\n",
      "circumstances\n",
      "manuscript\n",
      "simultaneously\n",
      "classifying\n",
      "aligning\n",
      "extended\n",
      "lagged\n",
      "collective\n",
      "molecular\n",
      "kinetics\n",
      "success\n",
      "chemical\n",
      "modification\n",
      "reliably\n",
      "spaces\n",
      "beyond\n",
      "greedy\n",
      "restricted\n",
      "convexity\n",
      "smoothness\n",
      "uncovers\n",
      "reminiscent\n",
      "submodular\n",
      "maximization\n",
      "resurrecting\n",
      "sigmoid\n",
      "dynamical\n",
      "isometry\n",
      "practice\n",
      "initialization\n",
      "mean\n",
      "squared\n",
      "singular\n",
      "jacobian\n",
      "o\n",
      ")$\n",
      "exponential\n",
      "orthogonal\n",
      "dramatically\n",
      "remained\n",
      "unclear\n",
      "employing\n",
      "powerful\n",
      "nonlinearity\n",
      "intriguingly\n",
      "relu\n",
      "incapable\n",
      "sigmoidal\n",
      "properly\n",
      "initialized\n",
      "reveals\n",
      "controlling\n",
      "nearly\n",
      "exploration\n",
      "exploitation\n",
      "thresholds\n",
      "formulations\n",
      "horizon\n",
      "discounted\n",
      "undiscounted\n",
      "illustrated\n",
      "analytic\n",
      "figures\n",
      "footfall\n",
      "demographic\n",
      "heat\n",
      "gender\n",
      "remaining\n",
      "visit\n",
      "hour\n",
      "experiences\n",
      "sales\n",
      "manage\n",
      "ip\n",
      "opencv\n",
      "raspberrypi\n",
      "outliers\n",
      "longest\n",
      "leg\n",
      "llpd\n",
      "elongated\n",
      "irregularly\n",
      "shaped\n",
      "eigengap\n",
      "statistic\n",
      "determines\n",
      "mislabeled\n",
      "ultrametric\n",
      "multiscale\n",
      "seismic\n",
      "densely\n",
      "risks\n",
      "geologic\n",
      "carbon\n",
      "sequestration\n",
      "migration\n",
      "fluids\n",
      "formations\n",
      "subsurface\n",
      "leakage\n",
      "monitoring\n",
      "geophysical\n",
      "indirect\n",
      "requiring\n",
      "interpretation\n",
      "yielding\n",
      "package\n",
      "analog\n",
      "site\n",
      "chimay\n",
      "\\'\n",
      "mexico\n",
      "eruptions\n",
      "geysers\n",
      "mathrm\n",
      "{\n",
      "_\n",
      "\\\n",
      "}$.\n",
      "formulating\n",
      "window\n",
      "slid\n",
      "throughout\n",
      "classify\n",
      "geyser\n",
      "recall\n",
      "889\n",
      "923\n",
      "}$\n",
      "solvers\n",
      "decisions\n",
      "choco\n",
      "eclipse\n",
      "gecode\n",
      "minion\n",
      "queens\n",
      "golomb\n",
      "ruler\n",
      "magic\n",
      "square\n",
      "golfers\n",
      "balanced\n",
      "block\n",
      "modelled\n",
      "solved\n",
      "solver\n",
      "meant\n",
      "indication\n",
      "influence\n",
      "specialised\n",
      "briefly\n",
      "polynomial\n",
      "decay\n",
      "eigenvalues\n",
      "integral\n",
      "tikhonov\n",
      "regularizer\n",
      "monotone\n",
      "minimax\n",
      "frank\n",
      "wolfe\n",
      "gained\n",
      "tremendous\n",
      "communities\n",
      "fairly\n",
      "leverage\n",
      "provably\n",
      "translate\n",
      "aga\n",
      "attributed\n",
      "synthesize\n",
      "synthesized\n",
      "access\n",
      "heavily\n",
      "primarily\n",
      "augment\n",
      "decoder\n",
      "synthesis\n",
      "sun\n",
      "rgb\n",
      "inner\n",
      "semi\n",
      "projections\n",
      "mahalanobis\n",
      "notion\n",
      "maximizing\n",
      "discriminatory\n",
      "riemannian\n",
      "projected\n",
      "availability\n",
      "deem\n",
      "sharing\n",
      "pulling\n",
      "away\n",
      "coming\n",
      "handcrafted\n",
      "benefitting\n",
      "cone\n",
      "constrained\n",
      "pursuit\n",
      "mp\n",
      "fw\n",
      "regained\n",
      "popularity\n",
      "intermediate\n",
      "conic\n",
      "atom\n",
      "principled\n",
      "definitions\n",
      "sublinear\n",
      "mathcal\n",
      "}(\n",
      ")$)\n",
      "smooth\n",
      "^{-\n",
      "})$)\n",
      "strongly\n",
      "correspondence\n",
      "reaction\n",
      "diffusion\n",
      "interacting\n",
      "central\n",
      "spatial\n",
      "superposition\n",
      "quad\n",
      "partitioned\n",
      "formulas\n",
      "positive\n",
      "integrate\n",
      "fire\n",
      "flashover\n",
      "occurs\n",
      "spreads\n",
      "crevices\n",
      "intense\n",
      "flashovers\n",
      "frightening\n",
      "phenomena\n",
      "who\n",
      "encounter\n",
      "firefighters\n",
      "safety\n",
      "lives\n",
      "smoke\n",
      "rollover\n",
      "(\"\n",
      "angel\n",
      "fingers\n",
      "\")\n",
      "firefighter\n",
      "predicted\n",
      "early\n",
      "55\n",
      "occurred\n",
      "stimuli\n",
      "primate\n",
      "ait\n",
      "cadieu\n",
      "dnns\n",
      "rival\n",
      "inferotemporal\n",
      "cortex\n",
      "lehky\n",
      "2011\n",
      "outstanding\n",
      "worthwhile\n",
      "investigating\n",
      ".'\n",
      "kurtosis\n",
      "pareto\n",
      "selectivity\n",
      "sparseness\n",
      "fundamentally\n",
      "conclusions\n",
      "alter\n",
      "ascendancy\n",
      "last\n",
      "categorization\n",
      "obtaining\n",
      "discrepancy\n",
      "shed\n",
      "outcomes\n",
      "sarcasm\n",
      "crucial\n",
      "robot\n",
      "intuition\n",
      "intuitive\n",
      "undeniably\n",
      "extraordinary\n",
      "greatest\n",
      "communicative\n",
      "robots\n",
      "sarcastic\n",
      "informed\n",
      "schema\n",
      "ridden\n",
      "cues\n",
      "good\n",
      "benchmarking\n",
      "engaging\n",
      "reparameterizing\n",
      "birkhoff\n",
      "polytope\n",
      "variational\n",
      "permutation\n",
      "sorting\n",
      "ranking\n",
      "reasoning\n",
      "permutations\n",
      "grows\n",
      "factorially\n",
      "poses\n",
      "surmount\n",
      "start\n",
      "usual\n",
      "relaxing\n",
      "doubly\n",
      "transformations\n",
      "invertible\n",
      "differentiable\n",
      "stick\n",
      "breaking\n",
      "unconstrained\n",
      "rounds\n",
      "vertices\n",
      "temperature\n",
      "limit\n",
      "concentrates\n",
      "densities\n",
      "reparameterization\n",
      "meets\n",
      "received\n",
      "gp\n",
      "essentially\n",
      "kernel\n",
      "translates\n",
      "inequality\n",
      "implies\n",
      "excel\n",
      "generator\n",
      "correspond\n",
      "steepest\n",
      "descent\n",
      "prominent\n",
      "goodfellow\n",
      "wgan\n",
      "formally\n",
      "requirements\n",
      "mini\n",
      "approximates\n",
      "wasserstein\n",
      "accompanying\n",
      "fulfills\n",
      "verify\n",
      "celeba\n",
      "billion\n",
      "waveform\n",
      "carries\n",
      "auditory\n",
      "phonological\n",
      "said\n",
      "itself\n",
      "acoustics\n",
      "converting\n",
      "characteristic\n",
      "enormous\n",
      "exceptional\n",
      "weaknesses\n",
      "vec2topic\n",
      "identifies\n",
      "captured\n",
      "list\n",
      "dirichlet\n",
      "emails\n",
      "chats\n",
      "diffused\n",
      "equally\n",
      "reports\n",
      "corpora\n",
      "static\n",
      "pca\n",
      "+\n",
      "decomposition\n",
      "review\n",
      "principal\n",
      "rpca\n",
      "refers\n",
      "candes\n",
      "wright\n",
      "li\n",
      "ma\n",
      "decomposing\n",
      "column\n",
      "practically\n",
      "version\n",
      "lying\n",
      "changing\n",
      "exhaustive\n",
      "decade\n",
      "counterpart\n",
      "describing\n",
      "discussing\n",
      "pros\n",
      "cons\n",
      "comparisons\n",
      "interpolating\n",
      "entries\n",
      "harmonics\n",
      "stationary\n",
      "trends\n",
      "lti\n",
      "mixtures\n",
      "recovers\n",
      "hmm\n",
      "obey\n",
      "stated\n",
      "packages\n",
      "oblivious\n",
      "services\n",
      "specification\n",
      "composition\n",
      "annotate\n",
      "interpretable\n",
      "descriptions\n",
      "automatize\n",
      "declarative\n",
      "svms\n",
      "land\n",
      "cover\n",
      "roots\n",
      "prominence\n",
      "sensing\n",
      "1a1\n",
      "1aa\n",
      "implication\n",
      "predisposed\n",
      "unclassified\n",
      "authors\n",
      "ultimately\n",
      "boils\n",
      "personal\n",
      "preference\n",
      "uniqueness\n",
      "counting\n",
      "tiny\n",
      "hu\n",
      "ramanan\n",
      "deeply\n",
      "extending\n",
      "subject\n",
      "biologically\n",
      "basal\n",
      "ganglia\n",
      "paradoxes\n",
      "nuclei\n",
      "assumed\n",
      "largely\n",
      "internal\n",
      "globus\n",
      "pallidus\n",
      "paradoxical\n",
      "anatomical\n",
      "inhibitory\n",
      "electrophys\n",
      "iological\n",
      "recordings\n",
      "reconciles\n",
      "pallidal\n",
      "plausible\n",
      "parameterization\n",
      "optimized\n",
      "collection\n",
      "electrophysiological\n",
      "respecting\n",
      "intrapallidal\n",
      "globally\n",
      "aforementioned\n",
      "reconciled\n",
      "weakly\n",
      "afferent\n",
      "unbalanced\n",
      "strongest\n",
      "excitation\n",
      "stn\n",
      "weakest\n",
      "inhibition\n",
      "striatum\n",
      "focused\n",
      "examining\n",
      "tsvms\n",
      "?\n",
      "glad\n",
      "diagnostic\n",
      "know\n",
      "expensive\n",
      "insufficient\n",
      "transductive\n",
      "unlabelled\n",
      "microarray\n",
      "prune\n",
      "superfluous\n",
      "rfe\n",
      "supposed\n",
      "avoid\n",
      "examined\n",
      "tsvm\n",
      "surpassed\n",
      "progresses\n",
      "nevertheless\n",
      "steadily\n",
      "pushes\n",
      "envelope\n",
      "voxel\n",
      "synthesizing\n",
      "interpolation\n",
      "subsequent\n",
      "extrapolation\n",
      "newer\n",
      "hallucinate\n",
      "blurry\n",
      "advantages\n",
      "flowing\n",
      "supervision\n",
      "dropping\n",
      "resolution\n",
      "quantitatively\n",
      "qualitatively\n",
      "estimators\n",
      "verified\n",
      "constant\n",
      "arbitrarily\n",
      "worse\n",
      "bpemb\n",
      "tokenization\n",
      "subword\n",
      "275\n",
      "unit\n",
      "byte\n",
      "encoding\n",
      "bpe\n",
      "grained\n",
      "typing\n",
      "competitively\n",
      "bet\n",
      "ter\n",
      "vastly\n",
      "fewer\n",
      "com\n",
      "bheinzerling\n",
      "usage\n",
      "mission\n",
      "critical\n",
      "voices\n",
      "rising\n",
      "textit\n",
      "limits\n",
      "rise\n",
      "neuromorphic\n",
      "tolerates\n",
      "failures\n",
      "synapses\n",
      "assessing\n",
      "quixotic\n",
      "venture\n",
      "hits\n",
      "impossibility\n",
      "crashes\n",
      "pessimistic\n",
      "lipschitz\n",
      "coefficient\n",
      "illustrating\n",
      "matches\n",
      "crash\n",
      "neither\n",
      "configurations\n",
      "nor\n",
      "impossible\n",
      "geodesic\n",
      "saliency\n",
      "exploited\n",
      "dedicated\n",
      "tunneling\n",
      "tackle\n",
      "textures\n",
      "chaotic\n",
      "validated\n",
      "cut\n",
      "simply\n",
      "attained\n",
      "highest\n",
      "f\n",
      "editing\n",
      "grounding\n",
      "talking\n",
      "parties\n",
      "engage\n",
      "reinforced\n",
      "guidance\n",
      "cooperatively\n",
      "accomplishing\n",
      "guesswhat\n",
      "?!\n",
      "engineering\n",
      "interpretability\n",
      "conversations\n",
      "indicates\n",
      "ended\n",
      "ambiguity\n",
      "lexica\n",
      "disambiguation\n",
      "employs\n",
      "verb\n",
      "enriched\n",
      "condensed\n",
      "induction\n",
      "arguments\n",
      "lack\n",
      "allowing\n",
      "invariants\n",
      "indexed\n",
      "progressively\n",
      "multidimensional\n",
      "convolutions\n",
      "averaging\n",
      "indices\n",
      "precisions\n",
      "man\n",
      "evolutions\n",
      "surveying\n",
      "devoted\n",
      "handling\n",
      "just\n",
      "paid\n",
      "improving\n",
      "lse\n",
      "lses\n",
      "compromised\n",
      "laborious\n",
      "replaced\n",
      "mathematically\n",
      "expedite\n",
      "keyword\n",
      "multitopic\n",
      "documents\n",
      "sources\n",
      "portals\n",
      "forums\n",
      "represented\n",
      "occurring\n",
      "centrality\n",
      "filtering\n",
      "grading\n",
      "questions\n",
      "education\n",
      "assessments\n",
      "homework\n",
      "kinds\n",
      "figure\n",
      "prominently\n",
      "stem\n",
      "courses\n",
      "mlp\n",
      "learners\n",
      "correctness\n",
      "inspiration\n",
      "comprises\n",
      "convert\n",
      "uncover\n",
      "incorrect\n",
      "nonparametrics\n",
      "grade\n",
      "assigned\n",
      "instructor\n",
      "bonus\n",
      "multistep\n",
      "departs\n",
      "mooc\n",
      "educational\n",
      "platforms\n",
      "monotonic\n",
      "sensible\n",
      "minimisation\n",
      "formalised\n",
      "researchers\n",
      "basic\n",
      "qualification\n",
      "ramification\n",
      "formalisation\n",
      "lie\n",
      "expositions\n",
      "blaming\n",
      "vehicle\n",
      "accidents\n",
      "responsibility\n",
      "automation\n",
      "harms\n",
      "someone\n",
      "blame\n",
      "causal\n",
      "drivers\n",
      "pedestrian\n",
      "hit\n",
      "killed\n",
      "operated\n",
      "primary\n",
      "causally\n",
      "responsible\n",
      "harm\n",
      "regardless\n",
      "portends\n",
      "malfunctioning\n",
      "cars\n",
      "regulatory\n",
      "operates\n",
      "tort\n",
      "law\n",
      "adjudicated\n",
      "jury\n",
      "regulate\n",
      "federal\n",
      "nuisance\n",
      "variability\n",
      "ratio\n",
      "isolate\n",
      "deployed\n",
      "ratios\n",
      "boxes\n",
      "determined\n",
      "inferior\n",
      "proper\n",
      "marginalization\n",
      "converse\n",
      "conclude\n",
      "marginalizing\n",
      "imagenet\n",
      "oxford\n",
      "fischer\n",
      "partitions\n",
      "relaxed\n",
      "rearrangement\n",
      "decreasing\n",
      "iterations\n",
      "handwritten\n",
      "digits\n",
      "manifold\n",
      "discretizations\n",
      "nested\n",
      "diminishing\n",
      "module\n",
      "theorem\n",
      "janhunen\n",
      "modular\n",
      "answer\n",
      "compositionality\n",
      "reactive\n",
      "ferraris\n",
      "applies\n",
      "count\n",
      "aggregate\n",
      "relating\n",
      "symmetric\n",
      "splitting\n",
      "reformulate\n",
      "bounded\n",
      "treewidth\n",
      "inferences\n",
      "anytime\n",
      "max\n",
      "thousands\n",
      "competitors\n",
      "demanding\n",
      "imputing\n",
      "forests\n",
      "tenth\n",
      "worst\n",
      "parallelizable\n",
      "instant\n",
      "bootstrapping\n",
      "serial\n",
      "parallelized\n",
      "replicated\n",
      "cumulative\n",
      "complimentary\n",
      "enhances\n",
      "identical\n",
      "iteration\n",
      "gazetteers\n",
      "wikipedia\n",
      "twnertc\n",
      "categorized\n",
      "crawler\n",
      "freebase\n",
      "approximately\n",
      "300k\n",
      "entities\n",
      "77\n",
      "loc\n",
      "org\n",
      "misc\n",
      "eventually\n",
      "truths\n",
      "annotators\n",
      "tc\n",
      "curiosity\n",
      "homeostatic\n",
      "regulation\n",
      "animal\n",
      "instinct\n",
      "added\n",
      "builds\n",
      "bellman\n",
      "equation\n",
      "orienteering\n",
      "ops\n",
      "prize\n",
      "collecting\n",
      "traveling\n",
      "salesman\n",
      "choose\n",
      "cities\n",
      "deadline\n",
      "travel\n",
      "sops\n",
      "trip\n",
      "suffer\n",
      "limitations\n",
      "route\n",
      "violating\n",
      "sop\n",
      "dsop\n",
      "preferences\n",
      "dsops\n",
      "park\n",
      "navigation\n",
      "proof\n",
      "s5\n",
      "ltl\n",
      "atl\n",
      "lesser\n",
      "mmlk\n",
      "perfect\n",
      "mps\n",
      "dis\n",
      "exhibited\n",
      "proofs\n",
      "bigger\n",
      "relates\n",
      "ao\n",
      "dfpn\n",
      "counterexample\n",
      "minimization\n",
      "origin\n",
      "mayan\n",
      "formosan\n",
      "austronesian\n",
      "names\n",
      "bbpns\n",
      "swadesh\n",
      "200\n",
      "cognates\n",
      "my\n",
      "vocabulary\n",
      "ca\n",
      "1300\n",
      "82\n",
      "listed\n",
      "tryon\n",
      "1985\n",
      "cgs\n",
      "table\n",
      "mscs\n",
      "mutually\n",
      "unrelated\n",
      "cg\n",
      "items\n",
      "giving\n",
      "closest\n",
      "css\n",
      "=\n",
      "33\n",
      "83\n",
      "western\n",
      "malayo\n",
      "polynesian\n",
      "shwng\n",
      "oceanic\n",
      ")=\n",
      "88\n",
      "...,\n",
      "oc\n",
      "csss\n",
      "underline\n",
      "portion\n",
      "section\n",
      "section1\n",
      "chi\n",
      "squar\n",
      "freedom\n",
      "eq\n",
      "]\n",
      "eqs\n",
      "revealed\n",
      "<\n",
      "001\n",
      "subgroups\n",
      "eskimo\n",
      "bbpn\n",
      "amlite\n",
      "amharic\n",
      "transliteration\n",
      "ascii\n",
      "transliterated\n",
      "letter\n",
      "interoperable\n",
      "transliterating\n",
      "letters\n",
      "indicated\n",
      "99\n",
      "98\n",
      "percentage\n",
      "exception\n",
      "conversion\n",
      "outlined\n",
      "rationales\n",
      "undertaken\n",
      "email\n",
      "spam\n",
      "arrive\n",
      "individually\n",
      "arrives\n",
      "lasso\n",
      "stage\n",
      "met\n",
      "massively\n",
      "logistic\n",
      "sphere\n",
      "normed\n",
      "treating\n",
      "cosine\n",
      "proximity\n",
      "300\n",
      "000\n",
      "imply\n",
      "contradiction\n",
      "defect\n",
      "dangerous\n",
      "limitation\n",
      "raise\n",
      "proposition\n",
      "concrete\n",
      "cellular\n",
      "``\n",
      "observer\n",
      "'',\n",
      "''\n",
      "''.\n",
      "roughly\n",
      "meaning\n",
      "enhanced\n",
      "ultrasound\n",
      "compressive\n",
      "deconvolution\n",
      "represents\n",
      "includes\n",
      "convolution\n",
      "carrying\n",
      "spread\n",
      "depends\n",
      "setup\n",
      "incoherence\n",
      "invert\n",
      "expressing\n",
      "reflectivity\n",
      "vivo\n",
      "restart\n",
      "cma\n",
      "es\n",
      "proceeds\n",
      "mutation\n",
      "doubling\n",
      "adaptively\n",
      "allocates\n",
      "bipop\n",
      "bbob\n",
      "generality\n",
      "suite\n",
      "spacecraft\n",
      "spatiotemporal\n",
      "anomaly\n",
      "cps\n",
      "cyber\n",
      "cpss\n",
      "faults\n",
      "anomalies\n",
      "vulnerable\n",
      "fault\n",
      "discovering\n",
      "subsystems\n",
      "boltzmann\n",
      "rbm\n",
      "nominal\n",
      "modes\n",
      "integrated\n",
      "highway\n",
      "reflective\n",
      "novelties\n",
      "multilevel\n",
      "shortcuts\n",
      "supports\n",
      "patches\n",
      "post\n",
      "pooling\n",
      "disparities\n",
      "outputs\n",
      "replaces\n",
      "winner\n",
      "refinement\n",
      "largest\n",
      "intractable\n",
      "rejection\n",
      "curse\n",
      "density\n",
      "innovations\n",
      "fits\n",
      "heteroscedastic\n",
      "burden\n",
      "genetics\n",
      "queueing\n",
      "betarun\n",
      "soccer\n",
      "league\n",
      "robocup\n",
      "official\n",
      "championships\n",
      "1997\n",
      "tactical\n",
      "richest\n",
      "behavioural\n",
      "competition\n",
      "attempt\n",
      "ultimate\n",
      "entirely\n",
      "observing\n",
      "playing\n",
      "agent2d\n",
      "lane\n",
      "targets\n",
      "kalman\n",
      "faithfully\n",
      "procured\n",
      "dictionaries\n",
      "generalizable\n",
      "mid\n",
      "cornerstone\n",
      "crucially\n",
      "codebook\n",
      "purpose\n",
      "collections\n",
      "reflects\n",
      "appearances\n",
      "chosen\n",
      "enough\n",
      "opening\n",
      "opportunity\n",
      "greatly\n",
      "alleviate\n",
      "confirming\n",
      "feasibility\n",
      "environments\n",
      "dial\n",
      "poorly\n",
      "acquired\n",
      "shift\n",
      "alleviating\n",
      "wild\n",
      "stems\n",
      "intuitively\n",
      "align\n",
      "resulted\n",
      "attempting\n",
      "promote\n",
      "introduction\n",
      "emph\n",
      "(\\\n",
      "reciprocity\n",
      "dyadic\n",
      "exciting\n",
      "hawkes\n",
      "history\n",
      "intensity\n",
      "interests\n",
      "individuals\n",
      "compound\n",
      "todeschini\n",
      "skolemization\n",
      "emerged\n",
      "eliminates\n",
      "existential\n",
      "subsets\n",
      "lifted\n",
      "counters\n",
      "skolem\n",
      "textbook\n",
      "restrictions\n",
      "precluded\n",
      "quantification\n",
      "simplifies\n",
      "personalized\n",
      "personalizable\n",
      "mdp\n",
      "viewfinder\n",
      "preview\n",
      "trade\n",
      "oscillation\n",
      "associate\n",
      "personalize\n",
      "adjustments\n",
      "device\n",
      "incorporating\n",
      "spot\n",
      "metering\n",
      "fivek\n",
      "iphone\n",
      "exhibits\n",
      "robotics\n",
      "manufacturers\n",
      "specifications\n",
      "commands\n",
      "affecting\n",
      "missions\n",
      "expand\n",
      "bandwidth\n",
      "reusability\n",
      "decrease\n",
      "unnecessary\n",
      "supplemental\n",
      "electrical\n",
      "cloud\n",
      "virtual\n",
      "vm\n",
      "raas\n",
      "rain\n",
      "masked\n",
      "subtracting\n",
      "selected\n",
      "raster\n",
      "scanning\n",
      "boundary\n",
      "artifacts\n",
      "cuts\n",
      "blending\n",
      "overlapping\n",
      "organizing\n",
      "corner\n",
      "fate\n",
      "motions\n",
      "appointed\n",
      "timinf\n",
      "portions\n",
      "semantically\n",
      "deducted\n",
      "sorties\n",
      "rte\n",
      "bisimulations\n",
      "equivalence\n",
      "elements\n",
      "conjunction\n",
      "cal\n",
      "bisimulation\n",
      "equivalences\n",
      "isomorphism\n",
      "consequence\n",
      "ufb\n",
      "closely\n",
      "duality\n",
      "overview\n",
      "nondeterministic\n",
      "specified\n",
      "cpts\n",
      "fragments\n",
      "situation\n",
      "construction\n",
      "asymmetry\n",
      "oriented\n",
      "parsimonious\n",
      "algebra\n",
      "cpt\n",
      "soundly\n",
      "dominates\n",
      "plücker\n",
      "coordinates\n",
      "orientation\n",
      "algebraic\n",
      "parameterize\n",
      "pl\n",
      "ucker\n",
      "squares\n",
      "mismatched\n",
      "comparably\n",
      "25\n",
      "les\n",
      "pomdp\n",
      "font\n",
      "meilleurs\n",
      "hackers\n",
      "tenir\n",
      "compte\n",
      "l\n",
      "incertitude\n",
      "dans\n",
      "penetration\n",
      "security\n",
      "executing\n",
      "hacking\n",
      "attacks\n",
      "doing\n",
      "costly\n",
      "attack\n",
      "intelligently\n",
      "expect\n",
      "pomdps\n",
      "adapted\n",
      "historians\n",
      "collectors\n",
      "tune\n",
      "771\n",
      "dated\n",
      "clothing\n",
      "flickr\n",
      "museum\n",
      "syntax\n",
      "unstructured\n",
      "executable\n",
      "asts\n",
      "dynamically\n",
      "paralleling\n",
      "hearthstone\n",
      "79\n",
      "bleu\n",
      "22\n",
      "%.\n",
      "atis\n",
      "jobs\n",
      "geo\n",
      "stabilization\n",
      "unmanned\n",
      "aerial\n",
      "uavs\n",
      "uav\n",
      "avoids\n",
      "necessity\n",
      "projective\n",
      "lieu\n",
      "phases\n",
      "50fps\n",
      "undertakes\n",
      "threading\n",
      "fitted\n",
      "pushed\n",
      "hints\n",
      "begun\n",
      "fooling\n",
      "retrain\n",
      "shortcoming\n",
      "classic\n",
      "misjudge\n",
      "overgeneralization\n",
      "serious\n",
      "obstacle\n",
      "reframing\n",
      "hypothetical\n",
      "elaboration\n",
      "overcomplete\n",
      "cool\n",
      "separable\n",
      "clipping\n",
      "neurocontrol\n",
      "minimise\n",
      "discretized\n",
      "truncated\n",
      "stops\n",
      "exactly\n",
      "terminal\n",
      "reached\n",
      "omitted\n",
      "optimum\n",
      "affects\n",
      "derivatives\n",
      "calculate\n",
      "dual\n",
      "similarly\n",
      "limiting\n",
      "retain\n",
      "encode\n",
      "zero\n",
      "unattainable\n",
      "linearity\n",
      "treed\n",
      "nonstationary\n",
      "compadagrad\n",
      "complementary\n",
      "adagrad\n",
      "incarnation\n",
      "diagonal\n",
      "prohibitive\n",
      "navigates\n",
      "^\n",
      "$.\n",
      "composite\n",
      "mirror\n",
      "ell_2\n",
      "saga\n",
      "langevin\n",
      "svrg\n",
      "variate\n",
      "underdamped\n",
      "hessian\n",
      "sharp\n",
      "regimes\n",
      "imperfect\n",
      "redundant\n",
      "deals\n",
      "contex\n",
      ">\n",
      "</\n",
      "consist\n",
      "installation\n",
      "situated\n",
      "manipulator\n",
      "pseudo\n",
      "originality\n",
      "imprecision\n",
      "redundancy\n",
      "reinforce\n",
      "lidar\n",
      "integrating\n",
      "clouds\n",
      "gps\n",
      "imu\n",
      "jointly\n",
      "carry\n",
      "maxf\n",
      "60x60\n",
      "meters\n",
      "smaller\n",
      "92\n",
      "60\n",
      "fill\n",
      "reflex\n",
      "close\n",
      "illuminating\n",
      "pedestrians\n",
      "urban\n",
      "infusion\n",
      "placed\n",
      "downstream\n",
      "detector\n",
      "occlusion\n",
      "caltech\n",
      "executes\n",
      "2x\n",
      "union\n",
      "intersections\n",
      "uoi\n",
      "realizing\n",
      "flexible\n",
      "intersection\n",
      "maintaining\n",
      "uoi_\n",
      "}$)\n",
      "electrophysiology\n",
      "genotype\n",
      "l1logistic\n",
      "cur\n",
      "parsimony\n",
      "factorization\n",
      "biomedical\n",
      "seamless\n",
      "senses\n",
      "impede\n",
      "understudied\n",
      "polarity\n",
      "inventory\n",
      "gradual\n",
      "cot\n",
      "initially\n",
      "afterwards\n",
      "c\n",
      "accurately\n",
      "continual\n",
      "branches\n",
      "outlier\n",
      "rest\n",
      "fraud\n",
      "customer\n",
      "hundreds\n",
      "dimensions\n",
      "none\n",
      "accepted\n",
      "viewpoint\n",
      "outlying\n",
      "ness\n",
      "fusing\n",
      "subspaces\n",
      "researches\n",
      "regarded\n",
      "soe1\n",
      "categorical\n",
      "scans\n",
      "appealing\n",
      "fastest\n",
      "perturb\n",
      "mpm\n",
      "perturbations\n",
      "visualize\n",
      "glioblastoma\n",
      "wrongly\n",
      "refine\n",
      "biomarkers\n",
      "resection\n",
      "volume\n",
      "especial\n",
      "concretely\n",
      "transformed\n",
      "mesh\n",
      "renderings\n",
      "chairs\n",
      "successfully\n",
      "cluttered\n",
      "reasonable\n",
      "going\n",
      "deeper\n",
      "multilingual\n",
      "banks\n",
      "mvso\n",
      "918\n",
      "spanish\n",
      "italian\n",
      "adjective\n",
      "noun\n",
      "anp\n",
      "detectors\n",
      "alexnet\n",
      "styled\n",
      "deepsentibank\n",
      "anps\n",
      "pool\n",
      "googlenet\n",
      "tag\n",
      "exploratory\n",
      "multifaceted\n",
      "purely\n",
      "exploiting\n",
      "tags\n",
      "faceted\n",
      "browsing\n",
      "kera\n",
      "articles\n",
      "extracts\n",
      "locate\n",
      "pertaining\n",
      "military\n",
      "buried\n",
      "minimalist\n",
      "grammars\n",
      "parsers\n",
      "left\n",
      "reading\n",
      "rewriting\n",
      "refining\n",
      "book\n",
      "lossless\n",
      "valuable\n",
      "compress\n",
      "taking\n",
      "compressor\n",
      "occam\n",
      "demarcation\n",
      "argues\n",
      "rigor\n",
      "objectivity\n",
      "argument\n",
      "plagued\n",
      "justify\n",
      "quantity\n",
      "gb\n",
      "mb\n",
      "delivered\n",
      "deploying\n",
      "devices\n",
      "decompositions\n",
      "speeding\n",
      "removing\n",
      "kernels\n",
      "optimizer\n",
      "speedup\n",
      "sometimes\n",
      "delivers\n",
      "counterparts\n",
      "nin\n",
      "91\n",
      "\\%$\n",
      "ilsvrc12\n",
      "vgg\n",
      "half\n",
      "urinary\n",
      "particles\n",
      "urine\n",
      "assist\n",
      "physicians\n",
      "renal\n",
      "tract\n",
      "labor\n",
      "consuming\n",
      "treat\n",
      "ssd\n",
      "involving\n",
      "comprehensively\n",
      "376\n",
      "erythrocyte\n",
      "leukocyte\n",
      "epithelial\n",
      "cell\n",
      "crystal\n",
      "cast\n",
      "mycete\n",
      "72\n",
      "ms\n",
      "nvidia\n",
      "titan\n",
      "x\n",
      "tm\n",
      "cleaning\n",
      "autodesk\n",
      "nlp4tm\n",
      "barbu\n",
      "undirected\n",
      "mcmc\n",
      "|\n",
      "quantities\n",
      "exceptionally\n",
      "equilibrium\n",
      "intractability\n",
      "familiar\n",
      "unexplored\n",
      "coronary\n",
      "samplers\n",
      "generally\n",
      "loopy\n",
      "brief\n",
      "acceptable\n",
      "posteriors\n",
      "epistemic\n",
      "strategic\n",
      "calls\n",
      "transfinite\n",
      "tarski\n",
      "fixpoint\n",
      "notions\n",
      "rationalizability\n",
      "iterated\n",
      "dominated\n",
      "adverse\n",
      "drug\n",
      "reactions\n",
      "alendronate\n",
      "adr\n",
      "concerned\n",
      "adrs\n",
      "carried\n",
      "simvastatin\n",
      "side\n",
      "detected\n",
      "computerized\n",
      "integers\n",
      "embedded\n",
      "bitwidth\n",
      "wage\n",
      "discretize\n",
      "shifted\n",
      "dataflow\n",
      "scaling\n",
      "arduous\n",
      "integer\n",
      "somehow\n",
      "deploy\n",
      "accelerators\n",
      "chips\n",
      "demands\n",
      "counterfactual\n",
      "instrumental\n",
      "fueled\n",
      "observables\n",
      "recipe\n",
      "ml\n",
      "randomization\n",
      "conditionally\n",
      "iv\n",
      "resolves\n",
      "imposes\n",
      "customization\n",
      "outline\n",
      "optimizing\n",
      "sums\n",
      "chebyshev\n",
      "expansions\n",
      "determinant\n",
      "cubic\n",
      "adaptions\n",
      "sgd\n",
      "frameworks\n",
      "accumulated\n",
      "provable\n",
      "estimator\n",
      "randomizing\n",
      "degrees\n",
      "randomness\n",
      "estimations\n",
      "showcase\n",
      "kinectfusion\n",
      "regularize\n",
      "icp\n",
      "registration\n",
      "matched\n",
      "highlight\n",
      "%,\n",
      "53\n",
      "ate\n",
      "freiburg\n",
      "plant\n",
      "dispatch\n",
      "involvement\n",
      "accompanied\n",
      "ed\n",
      "fuel\n",
      "satisfying\n",
      "inefficient\n",
      "metaheuristic\n",
      "appa\n",
      "apso\n",
      "localization\n",
      "decays\n",
      "repeated\n",
      "occurrences\n",
      "repeating\n",
      "doorways\n",
      "associations\n",
      "ransac\n",
      "exclusively\n",
      "30hz\n",
      "bio\n",
      "pain\n",
      "comprised\n",
      "mas\n",
      "contours\n",
      "surf\n",
      "speeded\n",
      "supervisor\n",
      "explorator\n",
      "communicating\n",
      "inspire\n",
      "dima\n",
      "api\n",
      "ateji\n",
      "px\n",
      "java\n",
      "compiling\n",
      "compilation\n",
      "proven\n",
      "compile\n",
      "relational\n",
      "circuit\n",
      "answered\n",
      "kr\n",
      "?,\n",
      "circuits\n",
      "rigidly\n",
      "triangular\n",
      "triangle\n",
      "penalising\n",
      "neighbouring\n",
      "triangles\n",
      "compromises\n",
      "discretisation\n",
      "unbounded\n",
      "se\n",
      "initialisation\n",
      "successes\n",
      "focusing\n",
      "unlabeled\n",
      "iteratively\n",
      "ceal\n",
      "standing\n",
      "cacd\n",
      "256\n",
      "slam\n",
      "pyramid\n",
      "lighting\n",
      "lucas\n",
      "kanade\n",
      "schedule\n",
      "lapse\n",
      "hours\n",
      "looking\n",
      "comparability\n",
      "narrow\n",
      "balance\n",
      "controlled\n",
      "benchmarked\n",
      "sections\n",
      "monolingual\n",
      "pearson\n",
      "reliable\n",
      "optimising\n",
      "parametrised\n",
      "reconstruct\n",
      "depths\n",
      "watching\n",
      "dcn\n",
      "attracting\n",
      "compatible\n",
      "normals\n",
      "inside\n",
      "computes\n",
      "neighboring\n",
      "planar\n",
      "edges\n",
      "discontinuity\n",
      "preserve\n",
      "photometric\n",
      "outdoor\n",
      "nyuv2\n",
      "conjugate\n",
      "acceleration\n",
      "smoothing\n",
      "filters\n",
      "width\n",
      "neighborhood\n",
      "aggressive\n",
      "sweep\n",
      "blurring\n",
      "running\n",
      "freezing\n",
      "restarting\n",
      "20x\n",
      "bilateral\n",
      "5x\n",
      "investigates\n",
      "prescribed\n",
      "proportions\n",
      "propagated\n",
      "accordance\n",
      "syllogism\n",
      "chaining\n",
      "threshold\n",
      "defining\n",
      "linguistically\n",
      "remain\n",
      "fledged\n",
      "con\n",
      "quaint\n",
      "corresponds\n",
      "thought\n",
      "commonsense\n",
      "ved\n",
      "encodes\n",
      "decoded\n",
      "serve\n",
      "bypassed\n",
      "diversified\n",
      "normally\n",
      "hyperparameters\n",
      "interact\n",
      "interplay\n",
      "sports\n",
      "teams\n",
      "nhl\n",
      "hockey\n",
      "nba\n",
      "basketball\n",
      "constitutes\n",
      "attentional\n",
      "coherently\n",
      "colored\n",
      "proto\n",
      "relevance\n",
      "necessarily\n",
      "ease\n",
      "scalability\n",
      "media\n",
      "sites\n",
      "twitter\n",
      "facebook\n",
      "lda\n",
      "weakness\n",
      "concerns\n",
      "abundant\n",
      "yahoo\n",
      "!\n",
      "thlda\n",
      "incorporates\n",
      "microblogging\n",
      "ap\n",
      "rcv1\n",
      "characters\n",
      "composes\n",
      "whitespace\n",
      "translated\n",
      "interpreting\n",
      "benefit\n",
      "alleviates\n",
      "normative\n",
      "receptive\n",
      "idealized\n",
      "chromatic\n",
      "axiomatic\n",
      "shapes\n",
      "retina\n",
      "lgn\n",
      "v1\n",
      "mammals\n",
      "beliefs\n",
      "tailored\n",
      "linearizations\n",
      "relying\n",
      "pushing\n",
      "heteroscedasticity\n",
      "amounts\n",
      "modality\n",
      "prosodic\n",
      "conversational\n",
      "listeners\n",
      "disambiguate\n",
      "integrates\n",
      "transcribed\n",
      "pitch\n",
      "coupled\n",
      "accepts\n",
      "parse\n",
      "disfluencies\n",
      "attachment\n",
      "lagrangean\n",
      "ascent\n",
      "quadratic\n",
      "lagrange\n",
      "duals\n",
      "message\n",
      "500\n",
      "laskey\n",
      "lehner\n",
      "1989\n",
      "provan\n",
      "1990\n",
      "foundations\n",
      "develops\n",
      "thoroughly\n",
      "reiter\n",
      "kleer\n",
      "1987\n",
      "inoue\n",
      "1992\n",
      "breakthroughs\n",
      "monolithic\n",
      "realm\n",
      "things\n",
      "break\n",
      "tens\n",
      "billions\n",
      "tricks\n",
      "hierarchies\n",
      "brand\n",
      "logos\n",
      "multibox\n",
      "contents\n",
      "continously\n",
      "retraining\n",
      "logo\n",
      "ned\n",
      "resolving\n",
      "mentions\n",
      "kb\n",
      "skip\n",
      "gram\n",
      "relatedness\n",
      "anchor\n",
      "leveraging\n",
      "anchors\n",
      "conll\n",
      "85\n",
      "tac\n",
      "2010\n",
      "thesauruses\n",
      "standards\n",
      "adjustment\n",
      "labeling\n",
      "strict\n",
      "liquids\n",
      "claimed\n",
      "incredible\n",
      "liquid\n",
      "aggregating\n",
      "enabling\n",
      "loop\n",
      "controllers\n",
      "asymmetric\n",
      "item\n",
      "explicitly\n",
      "contrasts\n",
      "walks\n",
      "existent\n",
      "concise\n",
      "collaboration\n",
      "reductions\n",
      "76\n",
      "producing\n",
      "aiding\n",
      "setups\n",
      "beneficial\n",
      "covers\n",
      "angle\n",
      "processed\n",
      "tandem\n",
      "fed\n",
      "viterbi\n",
      "likelihoods\n",
      "angles\n",
      "^\\\n",
      "circ\n",
      "frontal\n",
      "employment\n",
      "knowing\n",
      "manipulate\n",
      "gaming\n",
      "deteriorate\n",
      "sharply\n",
      "goodhart\n",
      "pursue\n",
      "contestant\n",
      ".\"\n",
      "designs\n",
      "he\n",
      "favorable\n",
      "surprisingly\n",
      "solvable\n",
      "formulates\n",
      "analytical\n",
      "mdps\n",
      "lmdps\n",
      "z\n",
      "taxi\n",
      "connectives\n",
      "spirit\n",
      "makinson\n",
      "characterizes\n",
      "satisfy\n",
      "weakening\n",
      "sen\n",
      "alpha\n",
      "author\n",
      "nonmonotonic\n",
      "jlc\n",
      "definability\n",
      "soundness\n",
      "negation\n",
      "disjunction\n",
      "klm\n",
      "engesser\n",
      "gabbay\n",
      "predominantly\n",
      "eliminating\n",
      "copies\n",
      "saved\n",
      "interdependent\n",
      "easier\n",
      "mdrnns\n",
      "mdrnn\n",
      "overcome\n",
      "hf\n",
      "ctc\n",
      "fisher\n",
      "15\n",
      "boosted\n",
      "savings\n",
      "transit\n",
      "bilevel\n",
      "timal\n",
      "minimizes\n",
      "nash\n",
      "combing\n",
      "hooke\n",
      "jeeves\n",
      "sioux\n",
      "falls\n",
      "ance\n",
      "tac2008\n",
      "tac2009\n",
      "blind\n",
      "separation\n",
      "multicomponent\n",
      "bss\n",
      "ds\n",
      "dt\n",
      "separability\n",
      "impose\n",
      "dx\n",
      "satisfied\n",
      "sounds\n",
      "microphone\n",
      "reified\n",
      "tension\n",
      "exists\n",
      "expressivity\n",
      "reconcile\n",
      "let\n",
      "arity\n",
      "reifying\n",
      "letting\n",
      "survival\n",
      "seizure\n",
      "eeg\n",
      "seizures\n",
      "manifestations\n",
      "capturing\n",
      "spatially\n",
      "sensitivity\n",
      "proves\n",
      "electrode\n",
      "montage\n",
      "happen\n",
      "arises\n",
      "?\".\n",
      "neighbour\n",
      "acquiring\n",
      "neighbours\n",
      "examines\n",
      "29\n",
      "distinct\n",
      "reporting\n",
      "advocate\n",
      "claim\n",
      "recommend\n",
      "surveillance\n",
      "street\n",
      "scenes\n",
      "traditionally\n",
      "perceived\n",
      "shortcomings\n",
      "rbqi\n",
      "summation\n",
      "correlating\n",
      "ratings\n",
      "trapezoidal\n",
      "precipitation\n",
      "forecast\n",
      "hebrew\n",
      "bible\n",
      "visualizations\n",
      "masoretic\n",
      "chapters\n",
      "passages\n",
      "inspected\n",
      "synoptically\n",
      "highlighted\n",
      "isaiah\n",
      "1qisaa\n",
      "kings\n",
      "19\n",
      "parallels\n",
      "jeremiah\n",
      "chronicles\n",
      "superintelligence\n",
      "lessons\n",
      "computability\n",
      "surpassing\n",
      "brightest\n",
      "gifted\n",
      "minds\n",
      "scientists\n",
      "philosophers\n",
      "technologists\n",
      "revived\n",
      "discussion\n",
      "entailed\n",
      "origins\n",
      "neo\n",
      "fear\n",
      "containment\n",
      "executed\n",
      "something\n",
      "mis\n",
      "mil\n",
      "prototypes\n",
      "restrict\n",
      "l_p\n",
      "unary\n",
      "pairwise\n",
      "l_1\n",
      "raising\n",
      "unfortunately\n",
      "submodularity\n",
      "geq\n",
      "bionic\n",
      "eap\n",
      "muscles\n",
      "reality\n",
      "conjures\n",
      "fiction\n",
      "tv\n",
      "superhuman\n",
      "implanted\n",
      "portrayed\n",
      "fantastic\n",
      "seem\n",
      "electroactive\n",
      "polymers\n",
      "bionics\n",
      "displacement\n",
      "stimulation\n",
      "actuation\n",
      "materials\n",
      "changed\n",
      "piezoelectric\n",
      "polymer\n",
      "pvdf\n",
      "lifelike\n",
      "multidisciplinary\n",
      "initiated\n",
      "international\n",
      "projects\n",
      "1999\n",
      "challenged\n",
      "worldwide\n",
      "experts\n",
      "arm\n",
      "actuated\n",
      "wrestling\n",
      "opponent\n",
      "reviewed\n",
      "enzynet\n",
      "enzyme\n",
      "ever\n",
      "increasingly\n",
      "bank\n",
      "fold\n",
      "expansion\n",
      "enzymatic\n",
      "amino\n",
      "acid\n",
      "conserved\n",
      "predictor\n",
      "commission\n",
      "enzymes\n",
      "biochemical\n",
      "558\n",
      "78\n",
      "shervinea\n",
      "icc\n",
      "gray\n",
      "offset\n",
      "printing\n",
      "prepress\n",
      "converted\n",
      "cmyk\n",
      "cyan\n",
      "magenta\n",
      "yellow\n",
      "graycolor\n",
      "colors\n",
      "printer\n",
      "printed\n",
      "developedand\n",
      "sheet\n",
      "calibrated\n",
      "inductive\n",
      "refer\n",
      "differs\n",
      "ogis\n",
      "synthesizers\n",
      "operate\n",
      "querying\n",
      "cegis\n",
      "counterexamples\n",
      "varies\n",
      "versus\n",
      "universe\n",
      "teaching\n",
      "altogether\n",
      "memristive\n",
      "routinely\n",
      "inherently\n",
      "unreliable\n",
      "double\n",
      "layered\n",
      "outer\n",
      "plexiform\n",
      "opl\n",
      "ipl\n",
      "center\n",
      "transient\n",
      "glassy\n",
      "share\n",
      "slows\n",
      "flat\n",
      "approaching\n",
      "diffuses\n",
      "barrier\n",
      "crossing\n",
      "distinctive\n",
      "landscapes\n",
      "mondrian\n",
      "ensembles\n",
      "breiman\n",
      "batches\n",
      "now\n",
      "greater\n",
      "roy\n",
      "teh\n",
      "2009\n",
      "grown\n",
      "periodically\n",
      "ergodic\n",
      "follows\n",
      "homogeneity\n",
      "markovian\n",
      "again\n",
      "$\\{(\n",
      "x_i\n",
      "y_i\n",
      ")\\}^\n",
      "n_\n",
      "mathbb\n",
      "}^\n",
      "y\n",
      "uncorrelated\n",
      "infty\n",
      "$(\n",
      ")/\n",
      "leq\n",
      "kappa\n",
      "sqrt\n",
      "pq\n",
      "}/\\|\\\n",
      "sigma_\n",
      "xy\n",
      "}\\|\n",
      "_f\n",
      "tight\n",
      "unsatisfactory\n",
      "cohesion\n",
      "cohesive\n",
      "themselves\n",
      "statements\n",
      "partitional\n",
      "lloyd\n",
      "definite\n",
      "resnets\n",
      "delta\n",
      "cognizant\n",
      "incongruence\n",
      "disagreement\n",
      "indicative\n",
      "nuance\n",
      "aposteriori\n",
      "divergences\n",
      "gauge\n",
      "postulate\n",
      "dominant\n",
      "clutter\n",
      "symmetry\n",
      "superiority\n",
      "dogmatism\n",
      "behavioral\n",
      "dogmatic\n",
      "comments\n",
      "reddit\n",
      "posts\n",
      "crowdsourcing\n",
      "operationalize\n",
      "refrain\n",
      "signaling\n",
      "trait\n",
      "tend\n",
      "surrogate\n",
      "msrs\n",
      "cite\n",
      "regis2007srbf\n",
      "surfaces\n",
      "revisited\n",
      "picked\n",
      "correspondingly\n",
      "perturbing\n",
      ".}\n",
      "lacking\n",
      "standardized\n",
      "extrinsic\n",
      "relied\n",
      "attractive\n",
      "inexpensive\n",
      "summarize\n",
      "sustainable\n",
      "kaczmarz\n",
      "conditioning\n",
      "/\\\n",
      "mu\n",
      ")^\n",
      "reweighting\n",
      "dominating\n",
      "broadly\n",
      "bodies\n",
      "recast\n",
      "converge\n",
      "frobenius\n",
      "algebras\n",
      "protocols\n",
      "abramsky\n",
      "coecke\n",
      "lambek\n",
      "pregroup\n",
      "formalizing\n",
      "grammatical\n",
      "implicit\n",
      "sadrzadeh\n",
      "category\n",
      "pregroups\n",
      "monoidal\n",
      "functors\n",
      "atiyah\n",
      "baez\n",
      "dolan\n",
      "pavlovic\n",
      "vicary\n",
      "meanings\n",
      "accounted\n",
      "rgf\n",
      "m\n",
      "repetitions\n",
      "k_max\n",
      "differently\n",
      "pointed\n",
      "analogous\n",
      "translating\n",
      "consequently\n",
      "confirmed\n",
      "sought\n",
      "deviations\n",
      "frequencies\n",
      "deviation\n",
      "caused\n",
      "zipf\n",
      "simon\n",
      "outilex\n",
      "plate\n",
      "forme\n",
      "logicielle\n",
      "traitement\n",
      "textes\n",
      "écrits\n",
      "industry\n",
      "implementing\n",
      "lexicons\n",
      "xml\n",
      "format\n",
      "converters\n",
      "originating\n",
      "ladl\n",
      "lgpl\n",
      "lr\n",
      "license\n",
      "neighbourhood\n",
      "nilm\n",
      "big\n",
      "household\n",
      "disaggregation\n",
      "appliance\n",
      "minute\n",
      "month\n",
      "homes\n",
      "disaggregate\n",
      "consumption\n",
      "submetering\n",
      "infrastructure\n",
      "loads\n",
      "disaggregating\n",
      "fridge\n",
      "heating\n",
      "cooling\n",
      "74\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     lm \u001b[38;5;241m=\u001b[39m \u001b[43mKneserNeyLanguageModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#, smoothing=<...>)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     ppx \u001b[38;5;241m=\u001b[39m perplexity(lm, test_lines)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN = \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[38;5;124m, Perplexity = \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (n, ppx))\n",
      "Cell \u001b[0;32mIn[43], line 18\u001b[0m, in \u001b[0;36mKneserNeyLanguageModel.__init__\u001b[0;34m(self, lines, n, delta)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_grams_counts[n]:\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_grams_counts[n][prefix]:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs[prefix][token] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 47\u001b[0m, in \u001b[0;36mKneserNeyLanguageModel.get_prob\u001b[0;34m(self, prefix, token)\u001b[0m\n\u001b[1;32m     45\u001b[0m token_prefix_amount \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _token \u001b[38;5;129;01min\u001b[39;00m counts[prefix]:\n\u001b[0;32m---> 47\u001b[0m     token_prefix_counts_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcounts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m]\u001b[49m[_token]\n\u001b[1;32m     48\u001b[0m     token_prefix_amount \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(token)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for n in (1, 2, 3):\n",
    "    lm = KneserNeyLanguageModel(train_lines, n=n) #, smoothing=<...>)\n",
    "    ppx = perplexity(lm, test_lines)\n",
    "    print(\"N = %i, Perplexity = %.5f\" % (n, ppx))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
  "notebookPath": "seminar.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
