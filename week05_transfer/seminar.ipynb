{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (2.2.3)\n",
      "Requirement already satisfied: matplotlib in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (3.9.2)\n",
      "Requirement already satisfied: torch in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (2.4.1)\n",
      "Requirement already satisfied: numpy in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from matplotlib) (6.4.5)\n",
      "Requirement already satisfied: filelock in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas) (1.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/ant.korneev/Library/Python/3.9/lib/python/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas matplotlib torch numpy\n",
    "%pip install -q transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zriTdjauH8iQ",
    "outputId": "e25d0f3f-2879-4d80-8cad-f949cf5b8453"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ant.korneev/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ant.korneev/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQiRPWWHlSgv"
   },
   "source": [
    "### Using pre-trained transformers (2 points)\n",
    "_for fun and profit_\n",
    "\n",
    "There are many toolkits that let you access pre-trained transformer models, but the most powerful and convenient by far is [`huggingface/transformers`](https://github.com/huggingface/transformers). In this week's practice, you'll learn how to download, apply and modify pre-trained transformers for a range of tasks. Buckle up, we're going in!\n",
    "\n",
    "\n",
    "__Pipelines:__ if all you want is to apply a pre-trained model, you can do that in one line of code using pipeline. Huggingface/transformers has a selection of pre-configured pipelines for masked language modelling, sentiment classification, question aswering, etc. ([see full list here](https://huggingface.co/transformers/main_classes/pipelines.html))\n",
    "\n",
    "A typical pipeline includes:\n",
    "* pre-processing, e.g. tokenization, subword segmentation\n",
    "* a backbone model, e.g. bert finetuned for classification\n",
    "* output post-processing\n",
    "\n",
    "Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162,
     "referenced_widgets": [
      "e85584c419aa445285eecb482778c7ba",
      "5694548560b548dabed4b111766da60e",
      "7fceb5ec7dec415a9dfcd4261aeb4a3d",
      "1fd2361180bd49aa8e2012a5e379d0c1",
      "77441a81d25c40819ae5af99ad70f3c8",
      "48c3bc239f16407bbba2d36ac6e5aaae",
      "5c87b856ffeb4c1dbf2d4637065d26ef",
      "fbbe93e1d4a5493782b7a246f3cb1760",
      "c3e31f6d90a84af08ee33023be184849",
      "6099e17176dd4eef829ac7c45badc9e2",
      "7ea9a125c76543419007fd5083ed63cd",
      "b2786e8af69e490cba5048fca7a0a51e",
      "f07e54fd856f4efc99bedb44abcece1b",
      "e0d2ca01fd7042a88a5493323a61409c",
      "e07d888d412d43f8ac4cc5285de07772",
      "15227ca6cc5a4d219e0113da09926da4",
      "cf83aaa31a1a47419db2127456beae69",
      "1e463fce356b46859493cbd975163ef1",
      "4ecb5a9efc3c42dd8faf4af9424ad5cf",
      "3af4702b95484b8cabd891b18ffc71b1",
      "e9210c257b8a4091b86123271c8a5486",
      "7eb0bf256b8d4975b0615aa96f00bbe1",
      "fc132bfb7d6447ad9900af7b80806aa1",
      "ab6d124394214415a2580983c88bbb11",
      "8578b4d2279f40dc846fcdfc2df55742",
      "7213248ebe1f4e28a00a93eb33169170",
      "b1497c0d7c3b4c04a91302a2cbc96db1",
      "60e971374e5448d08e9698a5594ff60a",
      "6c8113bb76024debaf3256431cbefe35",
      "73d8ad019f044a6998e81273a922745a",
      "a445a35355424cb0b4c7abf3d4e6f32d",
      "d71a81e25f5b4f88b8361c62e2ab9f44",
      "4f084bbe71bb433ba0e7ee50fccb71cc",
      "5cff2bc12fec494e9e19b8a956186df6",
      "787781c894bd46c6990f2513d9f2c79c",
      "3f4eae6f79054676976f87b9524c4cf5",
      "237d34743ee64428a9d22ee51e0423f3",
      "b095428dcb8a4408854f854de54d5692",
      "1b76512711884ffb81d3c3505b8fd137",
      "2f53a3d1d0f44192beeb6ed8d40e762f",
      "1b194af3033342b0b04dc12b3e055ff2",
      "391b8f44109544328617fbc3a3c3b9fc",
      "6f36db95f6fb4b4687c7130fc23c4348",
      "82a35a7ae5e441a1b66534a3eab5e765"
     ]
    },
    "id": "rP1KFtvLlJHR",
    "outputId": "f4e9f426-6445-4d69-b7b7-dd083d7bdebf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ant.korneev/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ant.korneev/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9998860359191895}]\n"
     ]
    }
   ],
   "source": [
    "classifier = transformers.pipeline('sentiment-analysis', model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "print(classifier(\"BERT is amazing!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'arryn': 'As High as Honor.',\n",
    "    'baratheon': 'Ours is the fury.',\n",
    "    'stark': 'Winter is coming.',\n",
    "    'tyrell': 'Growing strong.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nYUNuyXMn5l9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'arryn': True, 'baratheon': False, 'stark': True, 'tyrell': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# YOUR CODE: predict sentiment for each noble house and create outputs dict\n",
    "# outputs = <YOUR CODE: dict (house name) : True if positive, False if negative>\n",
    "outputs = { house: classifier(statement)[0]['label'] == 'POSITIVE' for house, statement in data.items() }\n",
    "\n",
    "assert sum(outputs.values()) == 3 and outputs[base64.decodebytes(b'YmFyYXRoZW9u\\n').decode()] == False\n",
    "print(\"Well done!\")\n",
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRDhIH-XpSNo"
   },
   "source": [
    "You can also access vanilla Masked Language Model that was trained to predict masked words. Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "pa-8noIllRbZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P=0.99719 donald trump is the president of the united states.\n",
      "P=0.00024 donald duck is the president of the united states.\n",
      "P=0.00022 donald ross is the president of the united states.\n",
      "P=0.00020 donald johnson is the president of the united states.\n",
      "P=0.00018 donald wilson is the president of the united states.\n"
     ]
    }
   ],
   "source": [
    "mlm_model = transformers.pipeline('fill-mask', model=\"bert-base-uncased\")\n",
    "MASK = mlm_model.tokenizer.mask_token\n",
    "\n",
    "for hypo in mlm_model(f\"Donald {MASK} is the president of the united states.\"):\n",
    "  print(f\"P={hypo['score']:.5f}\", hypo['sequence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "9NxeG1Y5pwX1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.03702252358198166,\n",
       "  'token': 2462,\n",
       "  'token_str': 'ii',\n",
       "  'sequence': 'the soviet union was founded in year ii.'},\n",
       " {'score': 0.030535690486431122,\n",
       "  'token': 3386,\n",
       "  'token_str': '1945',\n",
       "  'sequence': 'the soviet union was founded in year 1945.'},\n",
       " {'score': 0.02763737551867962,\n",
       "  'token': 4585,\n",
       "  'token_str': '1917',\n",
       "  'sequence': 'the soviet union was founded in year 1917.'},\n",
       " {'score': 0.023243557661771774,\n",
       "  'token': 4271,\n",
       "  'token_str': '1918',\n",
       "  'sequence': 'the soviet union was founded in year 1918.'},\n",
       " {'score': 0.017537729814648628,\n",
       "  'token': 4085,\n",
       "  'token_str': '1949',\n",
       "  'sequence': 'the soviet union was founded in year 1949.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your turn: use bert to recall what year was the Soviet Union founded in\n",
    "mlm_model(f'The Soviet Union was founded in year {MASK}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJxRFzCSq903"
   },
   "source": [
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Huggingface offers hundreds of pre-trained models that specialize on different tasks. You can quickly find the model you need using [this list](https://huggingface.co/models).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "HRux8Qp2hkXr"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"Almost two-thirds of the 1.5 million people who viewed this liveblog had Googled to discover\n",
    " the latest on the Rosetta mission. They were treated to this detailed account by the Guardian’s science editor,\n",
    " Ian Sample, and astronomy writer Stuart Clark of the moment scientists landed a robotic spacecraft on a comet\n",
    " for the first time in history, and the delirious reaction it provoked at their headquarters in Germany.\n",
    "  “We are there. We are sitting on the surface. Philae is talking to us,” said one scientist.\n",
    "\"\"\"\n",
    "\n",
    "# Task: create a pipeline for named entity recognition, use task name 'ner' and search for the right model in the list\n",
    "ner_model = transformers.pipeline(task='ner', model='dslim/bert-base-NER')\n",
    "\n",
    "named_entities = ner_model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hf57MRzSiSON"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT: [{'entity': 'B-LOC', 'score': 0.79910487, 'index': 27, 'word': 'Rose', 'start': 112, 'end': 116}, {'entity': 'I-LOC', 'score': 0.95119274, 'index': 28, 'word': '##tta', 'start': 116, 'end': 119}, {'entity': 'B-ORG', 'score': 0.998223, 'index': 40, 'word': 'Guardian', 'start': 179, 'end': 187}, {'entity': 'B-PER', 'score': 0.9997613, 'index': 46, 'word': 'Ian', 'start': 207, 'end': 210}, {'entity': 'I-PER', 'score': 0.99978715, 'index': 47, 'word': 'Sam', 'start': 211, 'end': 214}, {'entity': 'I-PER', 'score': 0.99964595, 'index': 48, 'word': '##ple', 'start': 214, 'end': 217}, {'entity': 'B-PER', 'score': 0.9997831, 'index': 53, 'word': 'Stuart', 'start': 240, 'end': 246}, {'entity': 'I-PER', 'score': 0.9997482, 'index': 54, 'word': 'Clark', 'start': 247, 'end': 252}, {'entity': 'B-LOC', 'score': 0.9997228, 'index': 85, 'word': 'Germany', 'start': 413, 'end': 420}, {'entity': 'B-PER', 'score': 0.9963127, 'index': 99, 'word': 'Phil', 'start': 470, 'end': 474}, {'entity': 'I-PER', 'score': 0.98892516, 'index': 100, 'word': '##ae', 'start': 474, 'end': 476}]\n",
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "print('OUTPUT:', named_entities)\n",
    "word_to_entity = {item['word']: item['entity'] for item in named_entities}\n",
    "assert 'org' in word_to_entity.get('Guardian').lower() and 'per' in word_to_entity.get('Stuart').lower()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULMownz6sP9n"
   },
   "source": [
    "### The building blocks of a pipeline\n",
    "\n",
    "Huggingface also allows you to access its pipelines on a lower level. There are two main abstractions for you:\n",
    "* `Tokenizer` - converts from strings to token ids and back\n",
    "* `Model` - a pytorch `nn.Module` with pre-trained weights\n",
    "\n",
    "You can use such models as part of your regular pytorch code: insert is as a layer in your model, apply it to a batch of data, backpropagate, optimize, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KMJbV0QVsO0Q"
   },
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.AutoModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZgSPHKPRxG6U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids tensor([[ 101, 5355, 1010, 1045, 2572, 2115, 2269, 1012,  102,    0,    0,    0,\n",
      "            0,    0,    0],\n",
      "        [ 101, 2166, 2003, 2054, 6433, 2043, 2017, 1005, 2128, 5697, 2437, 2060,\n",
      "         3488, 1012,  102]])\n",
      "token_type_ids tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "attention_mask tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "Detokenized:\n",
      "[CLS] luke, i am your father. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] life is what happens when you ' re busy making other plans. [SEP]\n"
     ]
    }
   ],
   "source": [
    "lines = [\n",
    "    \"Luke, I am your father.\",\n",
    "    \"Life is what happens when you're busy making other plans.\",\n",
    "    ]\n",
    "\n",
    "# tokenize a batch of inputs. \"pt\" means [p]y[t]orch tensors\n",
    "tokens_info = tokenizer(lines, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "for key in tokens_info:\n",
    "    print(key, tokens_info[key])\n",
    "\n",
    "print(\"Detokenized:\")\n",
    "for i in range(2):\n",
    "    print(tokenizer.decode(tokens_info['input_ids'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MJkbHxERyfL4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8854, -0.4722, -0.9392,  ..., -0.8081, -0.6955,  0.8748],\n",
      "        [-0.9297, -0.5161, -0.9334,  ..., -0.9017, -0.7492,  0.9201]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can now apply the model to get embeddings\n",
    "with torch.no_grad():\n",
    "    out = model(**tokens_info)\n",
    "\n",
    "print(out['pooler_output'])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBERTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = transformers.AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.head = nn.Linear(768, 2)\n",
    "        \n",
    "    def forward(self, **kwargs):\n",
    "        out = self.bert(**tokens_info)['pooler_output']\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MyBERTClassifier()\n",
    "\n",
    "opt = torch.optim.Adam(clf.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyBERTClassifier(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.train(False) # disables propout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7581319212913513\n",
      "0.7323513627052307\n",
      "0.6935027241706848\n",
      "0.6938388347625732\n",
      "0.6932213306427002\n",
      "0.6931890249252319\n",
      "0.6932262182235718\n",
      "0.6932340860366821\n",
      "0.6941834688186646\n",
      "0.694220781326294\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100):\n",
    "    logits = clf(**tokens_info)\n",
    "    loss = F.cross_entropy(logits, torch.tensor([1, 0]))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Vij7Gc1wOaq"
   },
   "source": [
    "Transformers knowledge hub: https://huggingface.co/transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bwmTTyjUGqol"
   },
   "source": [
    "### Build-a-transformer (2 points)\n",
    "\n",
    "In this section, you will implement a transformer language model layer by layer, then use it to generate (hopefully) coherent text.\n",
    "\n",
    "To understand how these layers work, please check out our guide to transformers from [nlp course for you -> transformers](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html#transformer_intro).\n",
    "\n",
    "\n",
    "First, we download pre-trained weights for the [GPT2 model by OpenAI](https://openai.com/research/better-language-models) - a prominent model from 2019.\n",
    "\n",
    "\n",
    "\n",
    "Idea & code by: Ilya Beletsky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOcK0lGTGqol",
    "outputId": "131fbc38-d4af-4e3b-b87b-f4c1b15d3162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: ['h.0.attn.c_attn.bias', 'h.0.attn.c_attn.weight', 'h.0.attn.c_proj.bias', 'h.0.attn.c_proj.weight', 'h.0.ln_1.bias', 'h.0.ln_1.weight', 'h.0.ln_2.bias', 'h.0.ln_2.weight', 'h.0.mlp.c_fc.bias', 'h.0.mlp.c_fc.weight', 'h.0.mlp.c_proj.bias', 'h.0.mlp.c_proj.weight', 'h.1.attn.c_attn.bias', 'h.1.attn.c_attn.weight', 'h.1. ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7l/3qdkg7n17mj7g3w_q8yf5xzc0000gq/T/ipykernel_35426/965270890.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "state_dict = torch.load(hf_hub_download(\"gpt2\", filename=\"pytorch_model.bin\"))\n",
    "for key, value in tuple(state_dict.items()):\n",
    "    if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
    "        value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
    "    if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
    "        state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
    "\n",
    "print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mr0SUtQnGqom"
   },
   "source": [
    "In the next few cells, we shall implement the model layer by layer to make use of those weights.\n",
    "\n",
    "As you might recall, transformers contain two main layer types: attention and fully-connected layers.\n",
    "\n",
    "The fully connected layers are by far easier to understand, so we shall begin there:\n",
    "\n",
    "Please implement fully-connected layer __without residual or layer normalization__ (we'll add those in a bit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3Rh-6DX9Gqom"
   },
   "outputs": [],
   "source": [
    "class GeLUThatWasUsedInGPT2(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * x ** 3)))\n",
    "\n",
    "class FullyConnected(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(dim, 4 * dim)\n",
    "        self.gelu = GeLUThatWasUsedInGPT2()\n",
    "        self.c_proj = nn.Linear(4 * dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape = [batch_size, seq_length, dim]\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSVGKnHBGqom"
   },
   "source": [
    "Now, let's test that it works with GPT-2 weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "CoWjZwZkGqom"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seems legit!\n"
     ]
    }
   ],
   "source": [
    "mlp = FullyConnected(dim=768)\n",
    "mlp.load_state_dict({'c_fc.weight': state_dict['h.0.mlp.c_fc.weight'],\n",
    "                     'c_fc.bias': state_dict['h.0.mlp.c_fc.bias'],\n",
    "                     'c_proj.weight': state_dict['h.0.mlp.c_proj.weight'],\n",
    "                     'c_proj.bias': state_dict['h.0.mlp.c_proj.bias']})\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "x = torch.randn(1, 2, 768)  # [batch_size, sequence_length, dim]\n",
    "checksum = torch.sum(mlp(x) * x)\n",
    "assert abs(checksum.item() - 1282.3315) < 0.1, \"layer outputs do not match reference\"\n",
    "assert torch.allclose(mlp(x[:, (1, 0), :])[:, (1, 0), :], mlp(x)), \"mlp must be permutation-invariant\"\n",
    "print(\"Seems legit!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VbfCevRwGqom"
   },
   "source": [
    "Now, let's get to attention layers.\n",
    "\n",
    "Since GPT-2 needs to generate text from left to right, each generated token can only attend to tokens on the left (and itself). This kid of attention is called \"Masked\" self-attention, because it hides tokens to the right.\n",
    "\n",
    "As before, please implement masked self-attention __without layernorm or residual connections.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "T6j7M4hLGqon"
   },
   "outputs": [],
   "source": [
    "class MaskedSelfAttention(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int, seq_len: int, batch_size: int):\n",
    "        super().__init__()\n",
    "        self.c_attn = nn.Linear(dim, dim * 3)  # query + key + value, combined\n",
    "        self.c_proj = nn.Linear(dim, dim)  # output projection\n",
    "        self.dim, self.num_heads, self.seq_len, self.batch_size = dim, num_heads, seq_len, batch_size\n",
    "        self.head_size = dim // num_heads # 64\n",
    "        self.attn_mask = self.get_attn_mask()\n",
    "        \n",
    "    def get_attn_mask(self):\n",
    "        heads_total = self.batch_size * self.num_heads\n",
    "        mask = torch.zeros([heads_total, heads_total], dtype=torch.bool)\n",
    "        \n",
    "        i = 0\n",
    "        step = 1\n",
    "        while i < len(mask):\n",
    "            for j in range(0, self.num_heads * step):\n",
    "                for k in range(self.num_heads):\n",
    "                    mask[i + k][j] = True\n",
    "            \n",
    "            step += 1\n",
    "            i += self.num_heads\n",
    "            \n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        # q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
    "        \n",
    "        # assert q.shape == k.shape == v.shape == x.shape, \"q, k and v must have the same shape as x\"\n",
    "\n",
    "        # q = q.reshape(batch_size, seq_len * self.num_heads, self.head_size)\n",
    "        # k = k.reshape(batch_size, seq_len * self.num_heads, self.head_size)\n",
    "        # v = v.reshape(batch_size, seq_len * self.num_heads, self.head_size)\n",
    "        \n",
    "        # # attention_layer\n",
    "        # scale_factor = 1 / math.sqrt(q.size(-1))\n",
    "        # attn_weight = (q @ k.transpose(1, 2)) * scale_factor\n",
    "        # # attn_weight = torch.where(self.attn_mask > 0, attn_weight, float(\"-inf\"))\n",
    "        # attn_weight = torch.softmax(attn_weight, dim=-1)\n",
    "        # attn_weight = attn_weight @ v\n",
    "        \n",
    "        # attn_output = self.c_proj(attn_weight)\n",
    "        \n",
    "        # return attn_output\n",
    "        \n",
    "        # Note: this is an inefficient implementation that uses a for-loop.\n",
    "        # To get the full grade during homework, please re-implement this code:\n",
    "        # 1) do not use for-loops (or other loops). Compute everything in parallel with vectorized operations\n",
    "        # 2) do not use F.scaled_dot_product_attention - write your own attention code using basic PyTorch ops\n",
    "        q, k, v = self.c_attn(x).split(dim=-1, split_size=self.dim)\n",
    "        \n",
    "        head_outputs = []\n",
    "        for head_index in range(self.num_heads):\n",
    "            head_selector = range(self.head_size * head_index, self.head_size * (head_index + 1))\n",
    "\n",
    "            head_queries: torch.Tensor = q[..., head_selector] # torch.Size([1, 10, 64])\n",
    "            \n",
    "            if head_index == 0:\n",
    "                _q = q.reshape(batch_size, seq_len * self.num_heads, self.head_size)\n",
    "                print(head_queries[0][0].sum(), _q[0][0].sum())\n",
    "                print(head_queries[0][1].sum(), _q[0][1].sum())\n",
    "                \n",
    "            \n",
    "            head_keys = k[..., head_selector]\n",
    "            head_values = v[..., head_selector]\n",
    "\n",
    "            single_head_output = F.scaled_dot_product_attention(\n",
    "                head_queries,\n",
    "                head_keys,\n",
    "                head_values,\n",
    "                is_causal=True\n",
    "            ) # запрещает токенам смотреть в будущее, так сделано маскирование\n",
    "            # docs: https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html\n",
    "            \n",
    "            head_outputs.append(single_head_output)\n",
    "\n",
    "        combined_head_outputs = torch.cat(head_outputs, dim=-1)\n",
    "        \n",
    "        # [batch_size, sequence_length, dim]\n",
    "        return self.c_proj(combined_head_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h_q torch.Size([1, 3, 2])\n",
      "h_q @ h_q.T torch.Size([1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "batch_size, seq_len, emb, num_heads = 1, 3, 4, 2\n",
    "head_dim = emb // num_heads\n",
    "q = torch.Tensor(\n",
    "        [\n",
    "            [\n",
    "                [1,2,3, 3.5],\n",
    "                [4,5,6, 6.5],\n",
    "                [7,8,9, 9.5]\n",
    "            ]\n",
    "        ]\n",
    "    )\n",
    "# print(q.reshape(batch_size, seq_len * num_heads, head_dim))\n",
    "\n",
    "h_q = q[..., range(0, num_heads)]\n",
    "h_q\n",
    "\n",
    "print('h_q', h_q.size())\n",
    "print('h_q @ h_q.T', (h_q @ h_q.transpose(-2, -1)).size())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umZpcpIkJva7"
   },
   "source": [
    "Test that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-42.7016, grad_fn=<SumBackward0>) tensor(-42.7016, grad_fn=<SumBackward0>)\n",
      "tensor(-17.8231, grad_fn=<SumBackward0>) tensor(-68.0790, grad_fn=<SumBackward0>)\n",
      "tensor(-277.8324, grad_fn=<SumBackward0>) tensor(-303.0928, grad_fn=<SumBackward0>)\n",
      "It works!\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
    "\n",
    "attn = MaskedSelfAttention(dim=768, num_heads=12, seq_len=x.size(1), batch_size=x.size(0))\n",
    "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
    "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
    "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
    "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "checksum = torch.sum(attn(x) * x)\n",
    "assert abs(checksum.item() - 2703.6772) < 0.1, checksum.item() # \"layer outputs do not match reference\"\n",
    "# assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
    "print(\"It works!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tg5Oj_PPM6hj",
    "outputId": "ebeddb50-d805-47ae-cc3a-4d68d900b3a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last weight of last head -1.5931510925292969 torch.Size([1, 10, 64])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "1657.6265869140625",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[304], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mmanual_seed(\u001b[38;5;241m1337\u001b[39m)\n\u001b[1;32m     10\u001b[0m checksum \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(attn(x) \u001b[38;5;241m*\u001b[39m x) \u001b[38;5;66;03m# -1.5931510925292969\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(checksum\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2703.6772\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.1\u001b[39m, checksum\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# \"layer outputs do not match reference\"\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt works!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: 1657.6265869140625"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 10, 768)  # [batch_size, sequence_length, dim]\n",
    "\n",
    "attn = MaskedSelfAttention(dim=768, num_heads=12, seq_len=x.size(1), batch_size=x.size(0))\n",
    "attn.load_state_dict({'c_attn.weight': state_dict['h.0.attn.c_attn.weight'],\n",
    "                      'c_attn.bias': state_dict['h.0.attn.c_attn.bias'],\n",
    "                      'c_proj.weight': state_dict['h.0.attn.c_proj.weight'],\n",
    "                      'c_proj.bias': state_dict['h.0.attn.c_proj.bias']})\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "checksum = torch.sum(attn(x) * x) # -1.5931510925292969\n",
    "assert abs(checksum.item() - 2703.6772) < 0.1, checksum.item() # \"layer outputs do not match reference\"\n",
    "# assert not torch.allclose(attn(x[:, (1, 0), :])[:, (1, 0), :], attn(x[:, (0, 1), :])), \"masked attention must *not* be permutation-invariant\"\n",
    "print(\"It works!\")\n",
    "\n",
    "# вес тот же без is_causal=True\n",
    "# этот вес не маскируется\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn6tgTHzOK4l"
   },
   "source": [
    "We can now combine attention and MLP to build the full transformer layer:\n",
    "\n",
    "![img](https://i.imgur.com/1sq2vHO.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "id": "p3AH7YQvRpvU"
   },
   "outputs": [],
   "source": [
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, dim: int, num_heads: int):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(dim)\n",
    "        self.attn = MaskedSelfAttention(dim, num_heads)\n",
    "        self.ln_2 = nn.LayerNorm(dim)\n",
    "        # multilayer perceptron\n",
    "        self.mlp = FullyConnected(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.attn(self.ln_1(x)) + x\n",
    "        print(attn.size())\n",
    "        \n",
    "        return self.mlp(self.ln_2(attn)) + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qzo_QeFVSNZa",
    "outputId": "15613968-b4d7-4391-dfff-3b490951a125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 768])\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "layer = TransformerLayer(dim=768, num_heads=12)\n",
    "layer.load_state_dict({k[5:]: v for k, v in state_dict.items() if k.startswith('h.10.')})\n",
    "assert abs(torch.sum(layer(x) * x).item() - 9874.7383) < 0.1, abs(torch.sum(layer(x) * x).item() - 9874.7383)\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mbqw9iuaSrYy"
   },
   "outputs": [],
   "source": [
    "class GPT2(nn.Module):\n",
    "    def __init__(self, vocab_size: int, dim: int, num_heads: int, num_layers: int, max_position_embeddings: int = 1024):\n",
    "        super().__init__()\n",
    "        # w for word\n",
    "        \n",
    "        # wte и на входе и на выходе\n",
    "        self.wte = nn.Embedding(vocab_size, dim)  # token embeddings\n",
    "        self.wpe = nn.Embedding(max_position_embeddings, dim)  # position embeddings\n",
    "        self.ln_f = nn.LayerNorm(dim)   # final layer norm - goes after all transformer layers, but before logits\n",
    "\n",
    "        # цикл\n",
    "        self.h = nn.Sequential(*(TransformerLayer(dim, num_heads) for layer in range(num_layers)))\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids.shape: [batch_size, sequence_length], int64 token ids\n",
    "        position_ids = torch.arange(input_ids.shape[1], device=input_ids.device).unsqueeze(0)\n",
    "\n",
    "        token_embeddings = self.wte(input_ids)\n",
    "        position_embeddings = self.wpe(position_ids)\n",
    "        full_embeddings = token_embeddings + position_embeddings\n",
    "\n",
    "        transformer_output = self.h(full_embeddings)\n",
    "        transformer_output_ln = self.ln_f(transformer_output)\n",
    "\n",
    "        # final layer: we predict logits by re-using token embeddings as linear weights\n",
    "        output_logits = transformer_output_ln @ self.wte.weight.T\n",
    "        return output_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import hf_hub_download\n",
    "# state_dict = torch.load(hf_hub_download(\"gpt2-large\", filename=\"pytorch_model.bin\"))\n",
    "# for key, value in tuple(state_dict.items()):\n",
    "#     if key.startswith('h.') and key.endswith('.weight') and value.ndim == 2:\n",
    "#         value.transpose_(1, 0)  # <-- for compatibility with modern PyTorch modules\n",
    "#     if key.startswith('h.') and key.endswith('.attn.bias') and value.ndim == 4:\n",
    "#         state_dict.pop(key)  # <-- triangular binar masks, not needed in this code\n",
    "\n",
    "# print('Weights:', repr(sorted(state_dict.keys()))[:320], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0m8jt66aDIh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction:  look\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = GPT2(vocab_size=50257, dim=1280, num_heads=20, num_layers=36)\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "input_ids = tokenizer(\"A quick\", return_tensors='pt')['input_ids']\n",
    "\n",
    "predicted_logits = model(input_ids)\n",
    "most_likely_token_id = predicted_logits[:, -1].argmax().item()\n",
    "\n",
    "print(\"Prediction:\", tokenizer.decode(most_likely_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8ql3Lo7dXZ2",
    "outputId": "8db86d13-d16b-4f97-db87-0385c0d91426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Fermi paradox  Search web here<|endoftext|>According to a sharing economy theory known as automantivizing , it posits\n",
      " that an object—or process—born on the network must be one with that object or process.\n",
      "\n",
      "Automotain doesn't require a organization\n",
      " to fall into a rule and make individual decisions based on what it believes an individual should be able to do—but many people\n",
      " likely don't trust their human abilities to support conformability.\n",
      "\n",
      "In other words, people need an organized account of\n",
      " themselves that governs their consumption, ownership, material feelings and behavior so they can \"know all behavior to be\n",
      " positive, regardless of others'. Over time, this information can become validity into a form that a benevolent system could\n",
      " eventually allow.\n",
      "\n",
      "However, intuition says that each unique mechanism worth riding into is determined and consistently improves\n",
      " over time so going from an organization that runs for living backward to a \"centered\" organization is inevitable.\n",
      "\n",
      "Thus,\n",
      " laser telephones are slowly becoming a deliberative vehicle. \"Think about the future versus the past. Your future selves\n",
      " truly require accurate decision making about what comes next for you. Shot any questions yet?\" The problem is that many guys\n",
      " probably don't need the boundaries they live by now, especially not into the \"carefree frontier. Do you own your own local\n",
      " parks or grow into a multi-utility automobile?\"\n",
      "\n",
      "Embrace to the scheme\n",
      "\n",
      "One of the best processes for sensory growth is the\n",
      " \"two moments\", exhaust signals of happiness and cross-entrance changes: one between \"impulse\" and \"outcome\" calls.\n",
      "\n",
      "by Robert\n",
      " Ney and next year's Christ Comprehensive Teaching Direct from Released\n",
      "\n",
      "When we reduce conditions in which we accumulate\n",
      " a horizon of various possible consequences, we distinguish between a feeling-based reactive pleasure organ/breathing machine\n",
      ", a motor for energy influx into little steps or impulses, and a neural signal for perceived sunderance. These synthetics\n",
      " get their calcium bodies pumped in on an irregular daily circuit to a volume in constant circulation.\n",
      "\n",
      "Aural patterns and\n",
      " mechanisms are complex and simply more efficient, facility, habit, planetary monogamy and the other basic wishes fulfilled\n",
      " with the aid of loss circuitry. Aaron Rask The Steve Jacob Teaching Address at Active Skills\n",
      "\n",
      "But what we don't mean by a\n",
      " two moments reaction, however, is \"all conditions are the same indefinitely\". To tell ourselves that from and opposition\n",
      ", no matter the \"1 I Love to get closer to\" condition"
     ]
    }
   ],
   "source": [
    "text = \"The Fermi paradox \"\n",
    "tokens = tokenizer.encode(text)\n",
    "print(end=tokenizer.decode(tokens))\n",
    "line_length = len(tokenizer.decode(tokens))\n",
    "\n",
    "for i in range(500):\n",
    "    # Predict logits with your model\n",
    "    with torch.no_grad():\n",
    "        logits = model(torch.as_tensor([tokens]))\n",
    "\n",
    "    # Sample with probabilities\n",
    "    p_next = torch.softmax(logits[0, -1, :], dim=-1).data.cpu().numpy()\n",
    "    next_token_index = np.random.choice(len(p_next), p=p_next)\n",
    "\n",
    "    tokens.append(int(next_token_index))\n",
    "    print(end=tokenizer.decode(tokens[-1]))\n",
    "    line_length += len(tokenizer.decode(tokens[-1]))\n",
    "    if line_length > 120:\n",
    "      line_length = 0\n",
    "      print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3NJ0ocgGqop"
   },
   "source": [
    "__Reminder:__ after class, please go to `MaskedSelfAttention.forward` above and finish the job!\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Here's how you can do the same with transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTOHu124Gqop",
    "outputId": "5bb38785-a7d9-47e1-a887-c03634945c0b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated continuation:  The Fermi paradox  (with its paradoxical consequences which, if any, may also be taken to be the paradox of the Big Bang. If an explosion can only happen after the collapse of the matter in one of three states  or after the collapse of a\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained('gpt2', add_prefix_space=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "print('Generated text:', tokenizer.decode(\n",
    "    model.generate(\n",
    "        **tokenizer(\"The Fermi paradox \", return_tensors='pt'),\n",
    "        do_sample=True, max_new_tokens=50\n",
    "    ).flatten().numpy()\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15227ca6cc5a4d219e0113da09926da4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b194af3033342b0b04dc12b3e055ff2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b76512711884ffb81d3c3505b8fd137": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e463fce356b46859493cbd975163ef1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fd2361180bd49aa8e2012a5e379d0c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6099e17176dd4eef829ac7c45badc9e2",
      "placeholder": "​",
      "style": "IPY_MODEL_7ea9a125c76543419007fd5083ed63cd",
      "value": " 629/629 [00:00&lt;00:00, 12.4kB/s]"
     }
    },
    "237d34743ee64428a9d22ee51e0423f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f36db95f6fb4b4687c7130fc23c4348",
      "placeholder": "​",
      "style": "IPY_MODEL_82a35a7ae5e441a1b66534a3eab5e765",
      "value": " 232k/232k [00:00&lt;00:00, 3.06MB/s]"
     }
    },
    "2f53a3d1d0f44192beeb6ed8d40e762f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "391b8f44109544328617fbc3a3c3b9fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3af4702b95484b8cabd891b18ffc71b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3f4eae6f79054676976f87b9524c4cf5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b194af3033342b0b04dc12b3e055ff2",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_391b8f44109544328617fbc3a3c3b9fc",
      "value": 231508
     }
    },
    "48c3bc239f16407bbba2d36ac6e5aaae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4ecb5a9efc3c42dd8faf4af9424ad5cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4f084bbe71bb433ba0e7ee50fccb71cc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5694548560b548dabed4b111766da60e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48c3bc239f16407bbba2d36ac6e5aaae",
      "placeholder": "​",
      "style": "IPY_MODEL_5c87b856ffeb4c1dbf2d4637065d26ef",
      "value": "Downloading (…)lve/main/config.json: 100%"
     }
    },
    "5c87b856ffeb4c1dbf2d4637065d26ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5cff2bc12fec494e9e19b8a956186df6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_787781c894bd46c6990f2513d9f2c79c",
       "IPY_MODEL_3f4eae6f79054676976f87b9524c4cf5",
       "IPY_MODEL_237d34743ee64428a9d22ee51e0423f3"
      ],
      "layout": "IPY_MODEL_b095428dcb8a4408854f854de54d5692"
     }
    },
    "6099e17176dd4eef829ac7c45badc9e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60e971374e5448d08e9698a5594ff60a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c8113bb76024debaf3256431cbefe35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f36db95f6fb4b4687c7130fc23c4348": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7213248ebe1f4e28a00a93eb33169170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d71a81e25f5b4f88b8361c62e2ab9f44",
      "placeholder": "​",
      "style": "IPY_MODEL_4f084bbe71bb433ba0e7ee50fccb71cc",
      "value": " 48.0/48.0 [00:00&lt;00:00, 1.58kB/s]"
     }
    },
    "73d8ad019f044a6998e81273a922745a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "77441a81d25c40819ae5af99ad70f3c8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "787781c894bd46c6990f2513d9f2c79c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1b76512711884ffb81d3c3505b8fd137",
      "placeholder": "​",
      "style": "IPY_MODEL_2f53a3d1d0f44192beeb6ed8d40e762f",
      "value": "Downloading (…)solve/main/vocab.txt: 100%"
     }
    },
    "7ea9a125c76543419007fd5083ed63cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7eb0bf256b8d4975b0615aa96f00bbe1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7fceb5ec7dec415a9dfcd4261aeb4a3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fbbe93e1d4a5493782b7a246f3cb1760",
      "max": 629,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c3e31f6d90a84af08ee33023be184849",
      "value": 629
     }
    },
    "82a35a7ae5e441a1b66534a3eab5e765": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8578b4d2279f40dc846fcdfc2df55742": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_73d8ad019f044a6998e81273a922745a",
      "max": 48,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a445a35355424cb0b4c7abf3d4e6f32d",
      "value": 48
     }
    },
    "a445a35355424cb0b4c7abf3d4e6f32d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab6d124394214415a2580983c88bbb11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60e971374e5448d08e9698a5594ff60a",
      "placeholder": "​",
      "style": "IPY_MODEL_6c8113bb76024debaf3256431cbefe35",
      "value": "Downloading (…)okenizer_config.json: 100%"
     }
    },
    "b095428dcb8a4408854f854de54d5692": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1497c0d7c3b4c04a91302a2cbc96db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2786e8af69e490cba5048fca7a0a51e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f07e54fd856f4efc99bedb44abcece1b",
       "IPY_MODEL_e0d2ca01fd7042a88a5493323a61409c",
       "IPY_MODEL_e07d888d412d43f8ac4cc5285de07772"
      ],
      "layout": "IPY_MODEL_15227ca6cc5a4d219e0113da09926da4"
     }
    },
    "c3e31f6d90a84af08ee33023be184849": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cf83aaa31a1a47419db2127456beae69": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d71a81e25f5b4f88b8361c62e2ab9f44": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e07d888d412d43f8ac4cc5285de07772": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e9210c257b8a4091b86123271c8a5486",
      "placeholder": "​",
      "style": "IPY_MODEL_7eb0bf256b8d4975b0615aa96f00bbe1",
      "value": " 268M/268M [00:01&lt;00:00, 172MB/s]"
     }
    },
    "e0d2ca01fd7042a88a5493323a61409c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4ecb5a9efc3c42dd8faf4af9424ad5cf",
      "max": 267832558,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3af4702b95484b8cabd891b18ffc71b1",
      "value": 267832558
     }
    },
    "e85584c419aa445285eecb482778c7ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5694548560b548dabed4b111766da60e",
       "IPY_MODEL_7fceb5ec7dec415a9dfcd4261aeb4a3d",
       "IPY_MODEL_1fd2361180bd49aa8e2012a5e379d0c1"
      ],
      "layout": "IPY_MODEL_77441a81d25c40819ae5af99ad70f3c8"
     }
    },
    "e9210c257b8a4091b86123271c8a5486": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f07e54fd856f4efc99bedb44abcece1b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cf83aaa31a1a47419db2127456beae69",
      "placeholder": "​",
      "style": "IPY_MODEL_1e463fce356b46859493cbd975163ef1",
      "value": "Downloading model.safetensors: 100%"
     }
    },
    "fbbe93e1d4a5493782b7a246f3cb1760": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc132bfb7d6447ad9900af7b80806aa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ab6d124394214415a2580983c88bbb11",
       "IPY_MODEL_8578b4d2279f40dc846fcdfc2df55742",
       "IPY_MODEL_7213248ebe1f4e28a00a93eb33169170"
      ],
      "layout": "IPY_MODEL_b1497c0d7c3b4c04a91302a2cbc96db1"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
